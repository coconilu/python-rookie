#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Session24 æ¼”ç¤ºé¡¹ç›®ï¼šé«˜æ€§èƒ½æ•°æ®å¤„ç†å™¨

è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„æ¼”ç¤ºé¡¹ç›®ï¼Œå±•ç¤ºäº†å¤šç§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„è¿ç”¨ï¼š
1. å¤§æ–‡ä»¶å¤„ç†ä¼˜åŒ–
2. å†…å­˜ç®¡ç†ä¼˜åŒ–
3. ç®—æ³•ä¼˜åŒ–
4. ç¼“å­˜ç­–ç•¥
5. å¹¶å‘å¤„ç†
6. æ€§èƒ½ç›‘æ§

é¡¹ç›®åœºæ™¯ï¼š
å¤„ç†å¤§é‡çš„é”€å”®æ•°æ®ï¼Œè¿›è¡Œç»Ÿè®¡åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import time
import csv
import json
import random
import threading
from datetime import datetime, timedelta
from collections import defaultdict, Counter, namedtuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import lru_cache
import gc
import tracemalloc
from pathlib import Path


# ä½¿ç”¨namedtupleä¼˜åŒ–å†…å­˜
SalesRecord = namedtuple('SalesRecord', [
    'date', 'product_id', 'category', 'quantity', 'price', 'customer_id', 'region'
])


class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§å™¨
    """
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.operations = []
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        gc.collect()
        tracemalloc.start()
        self.start_time = time.time()
        self.start_memory = tracemalloc.get_traced_memory()[0]
    
    def log_operation(self, operation_name):
        """è®°å½•æ“ä½œ"""
        current_time = time.time()
        current_memory = tracemalloc.get_traced_memory()[0]
        
        self.operations.append({
            'operation': operation_name,
            'time': current_time - self.start_time,
            'memory_mb': (current_memory - self.start_memory) / 1024 / 1024
        })
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æŠ¥å‘Š"""
        end_time = time.time()
        end_memory, peak_memory = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        total_time = end_time - self.start_time
        memory_used = (end_memory - self.start_memory) / 1024 / 1024
        peak_memory_used = (peak_memory - self.start_memory) / 1024 / 1024
        
        return {
            'total_time': total_time,
            'memory_used_mb': memory_used,
            'peak_memory_mb': peak_memory_used,
            'operations': self.operations
        }


class OptimizedDataProcessor:
    """
    ä¼˜åŒ–çš„æ•°æ®å¤„ç†å™¨
    """
    def __init__(self, cache_size=1000):
        self.cache_size = cache_size
        self.monitor = PerformanceMonitor()
        self._category_cache = {}
        self._region_cache = {}
        
        # çº¿ç¨‹é”ï¼Œç”¨äºçº¿ç¨‹å®‰å…¨
        self._lock = threading.Lock()
    
    def generate_sample_data(self, num_records=100000, filename="sales_data.csv"):
        """
        ç”Ÿæˆç¤ºä¾‹é”€å”®æ•°æ®
        """
        print(f"ç”Ÿæˆ {num_records:,} æ¡é”€å”®è®°å½•...")
        
        categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty']
        regions = ['North', 'South', 'East', 'West', 'Central']
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜ä½¿ç”¨
        def data_generator():
            base_date = datetime(2024, 1, 1)
            for i in range(num_records):
                date = base_date + timedelta(days=random.randint(0, 365))
                yield {
                    'date': date.strftime('%Y-%m-%d'),
                    'product_id': f'P{random.randint(1000, 9999)}',
                    'category': random.choice(categories),
                    'quantity': random.randint(1, 10),
                    'price': round(random.uniform(10, 1000), 2),
                    'customer_id': f'C{random.randint(10000, 99999)}',
                    'region': random.choice(regions)
                }
        
        # åˆ†æ‰¹å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
        batch_size = 10000
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'date', 'product_id', 'category', 'quantity', 'price', 'customer_id', 'region'
            ])
            writer.writeheader()
            
            batch = []
            for record in data_generator():
                batch.append(record)
                if len(batch) >= batch_size:
                    writer.writerows(batch)
                    batch = []
            
            # å†™å…¥å‰©ä½™è®°å½•
            if batch:
                writer.writerows(batch)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        return filename
    
    def read_data_optimized(self, filename):
        """
        ä¼˜åŒ–çš„æ•°æ®è¯»å–æ–¹æ³•ï¼ˆä½¿ç”¨ç”Ÿæˆå™¨ï¼‰
        """
        with open(filename, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è½¬æ¢ä¸ºnamedtupleä»¥èŠ‚çœå†…å­˜
                yield SalesRecord(
                    date=row['date'],
                    product_id=row['product_id'],
                    category=row['category'],
                    quantity=int(row['quantity']),
                    price=float(row['price']),
                    customer_id=row['customer_id'],
                    region=row['region']
                )
    
    def read_data_basic(self, filename):
        """
        åŸºç¡€çš„æ•°æ®è¯»å–æ–¹æ³•ï¼ˆå…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰
        """
        records = []
        with open(filename, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                records.append(SalesRecord(
                    date=row['date'],
                    product_id=row['product_id'],
                    category=row['category'],
                    quantity=int(row['quantity']),
                    price=float(row['price']),
                    customer_id=row['customer_id'],
                    region=row['region']
                ))
        return records
    
    @lru_cache(maxsize=128)
    def get_category_multiplier(self, category):
        """
        è·å–ç±»åˆ«ä¹˜æ•°ï¼ˆæ¨¡æ‹Ÿå¤æ‚è®¡ç®—ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
        """
        # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
        time.sleep(0.001)  # æ¨¡æ‹Ÿè®¡ç®—å»¶è¿Ÿ
        
        multipliers = {
            'Electronics': 1.2,
            'Clothing': 1.1,
            'Books': 0.9,
            'Home': 1.15,
            'Sports': 1.05,
            'Beauty': 1.3
        }
        return multipliers.get(category, 1.0)
    
    def calculate_revenue_optimized(self, records):
        """
        ä¼˜åŒ–çš„æ”¶å…¥è®¡ç®—ï¼ˆä½¿ç”¨ç”Ÿæˆå™¨å’Œç¼“å­˜ï¼‰
        """
        total_revenue = 0
        category_revenue = defaultdict(float)
        region_revenue = defaultdict(float)
        
        for record in records:
            # ä½¿ç”¨ç¼“å­˜çš„ç±»åˆ«ä¹˜æ•°
            multiplier = self.get_category_multiplier(record.category)
            revenue = record.quantity * record.price * multiplier
            
            total_revenue += revenue
            category_revenue[record.category] += revenue
            region_revenue[record.region] += revenue
        
        return {
            'total': total_revenue,
            'by_category': dict(category_revenue),
            'by_region': dict(region_revenue)
        }
    
    def calculate_revenue_basic(self, records):
        """
        åŸºç¡€çš„æ”¶å…¥è®¡ç®—
        """
        total_revenue = 0
        category_revenue = {}
        region_revenue = {}
        
        for record in records:
            # æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ä¹˜æ•°ï¼ˆæ— ç¼“å­˜ï¼‰
            if record.category == 'Electronics':
                multiplier = 1.2
            elif record.category == 'Clothing':
                multiplier = 1.1
            elif record.category == 'Books':
                multiplier = 0.9
            elif record.category == 'Home':
                multiplier = 1.15
            elif record.category == 'Sports':
                multiplier = 1.05
            elif record.category == 'Beauty':
                multiplier = 1.3
            else:
                multiplier = 1.0
            
            revenue = record.quantity * record.price * multiplier
            
            total_revenue += revenue
            
            # ä½¿ç”¨æ™®é€šå­—å…¸ï¼ˆéœ€è¦æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨ï¼‰
            if record.category in category_revenue:
                category_revenue[record.category] += revenue
            else:
                category_revenue[record.category] = revenue
            
            if record.region in region_revenue:
                region_revenue[record.region] += revenue
            else:
                region_revenue[record.region] = revenue
        
        return {
            'total': total_revenue,
            'by_category': category_revenue,
            'by_region': region_revenue
        }
    
    def analyze_customer_patterns(self, records):
        """
        åˆ†æå®¢æˆ·è´­ä¹°æ¨¡å¼
        """
        customer_stats = defaultdict(lambda: {
            'total_spent': 0,
            'order_count': 0,
            'categories': set(),
            'regions': set()
        })
        
        for record in records:
            customer = customer_stats[record.customer_id]
            customer['total_spent'] += record.quantity * record.price
            customer['order_count'] += 1
            customer['categories'].add(record.category)
            customer['regions'].add(record.region)
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for customer_id, stats in customer_stats.items():
            stats['categories'] = list(stats['categories'])
            stats['regions'] = list(stats['regions'])
            stats['avg_order_value'] = stats['total_spent'] / stats['order_count']
        
        return dict(customer_stats)
    
    def parallel_process_by_region(self, filename, max_workers=4):
        """
        å¹¶è¡Œå¤„ç†ä¸åŒåœ°åŒºçš„æ•°æ®
        """
        # é¦–å…ˆæŒ‰åœ°åŒºåˆ†ç»„æ•°æ®
        region_data = defaultdict(list)
        
        for record in self.read_data_optimized(filename):
            region_data[record.region].append(record)
        
        def process_region(region_records):
            """å¤„ç†å•ä¸ªåœ°åŒºçš„æ•°æ®"""
            region, records = region_records
            revenue = self.calculate_revenue_optimized(iter(records))
            customer_patterns = self.analyze_customer_patterns(records)
            
            return {
                'region': region,
                'revenue': revenue,
                'customer_count': len(customer_patterns),
                'record_count': len(records)
            }
        
        # å¹¶è¡Œå¤„ç†å„åœ°åŒº
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_region, item) for item in region_data.items()]
            results = [future.result() for future in futures]
        
        return results
    
    def generate_report(self, analysis_results):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_revenue': 0,
                'total_customers': 0,
                'total_records': 0
            },
            'regional_analysis': analysis_results,
            'performance_metrics': None
        }
        
        # æ±‡æ€»ç»Ÿè®¡
        for region_result in analysis_results:
            report['summary']['total_revenue'] += region_result['revenue']['total']
            report['summary']['total_customers'] += region_result['customer_count']
            report['summary']['total_records'] += region_result['record_count']
        
        return report
    
    def benchmark_methods(self, filename):
        """
        åŸºå‡†æµ‹è¯•ä¸åŒçš„å¤„ç†æ–¹æ³•
        """
        print("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
        
        methods = [
            ("åŸºç¡€æ–¹æ³•(å…¨å†…å­˜åŠ è½½)", self._benchmark_basic_method),
            ("ä¼˜åŒ–æ–¹æ³•(ç”Ÿæˆå™¨+ç¼“å­˜)", self._benchmark_optimized_method),
            ("å¹¶è¡Œå¤„ç†", self._benchmark_parallel_method)
        ]
        
        results = []
        
        for method_name, method_func in methods:
            print(f"\næµ‹è¯• {method_name}...")
            
            # æ¸…ç†ç¼“å­˜
            self.get_category_multiplier.cache_clear()
            gc.collect()
            
            try:
                result = method_func(filename)
                results.append((method_name, result))
                
                print(f"  æ‰§è¡Œæ—¶é—´: {result['execution_time']:.4f} ç§’")
                print(f"  å†…å­˜ä½¿ç”¨: {result['memory_used']:.2f} MB")
                print(f"  æ€»æ”¶å…¥: ${result['total_revenue']:,.2f}")
                
            except Exception as e:
                print(f"  âŒ æ‰§è¡Œå¤±è´¥: {e}")
        
        return results
    
    def _benchmark_basic_method(self, filename):
        """åŸºç¡€æ–¹æ³•åŸºå‡†æµ‹è¯•"""
        self.monitor.start_monitoring()
        
        # å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
        records = self.read_data_basic(filename)
        self.monitor.log_operation("æ•°æ®åŠ è½½")
        
        # åŸºç¡€è®¡ç®—
        revenue_result = self.calculate_revenue_basic(records)
        self.monitor.log_operation("æ”¶å…¥è®¡ç®—")
        
        performance = self.monitor.stop_monitoring()
        
        return {
            'execution_time': performance['total_time'],
            'memory_used': performance['memory_used_mb'],
            'total_revenue': revenue_result['total'],
            'method': 'basic'
        }
    
    def _benchmark_optimized_method(self, filename):
        """ä¼˜åŒ–æ–¹æ³•åŸºå‡†æµ‹è¯•"""
        self.monitor.start_monitoring()
        
        # ä½¿ç”¨ç”Ÿæˆå™¨
        records = self.read_data_optimized(filename)
        self.monitor.log_operation("æ•°æ®åŠ è½½(ç”Ÿæˆå™¨)")
        
        # ä¼˜åŒ–è®¡ç®—
        revenue_result = self.calculate_revenue_optimized(records)
        self.monitor.log_operation("æ”¶å…¥è®¡ç®—(ç¼“å­˜)")
        
        performance = self.monitor.stop_monitoring()
        
        return {
            'execution_time': performance['total_time'],
            'memory_used': performance['memory_used_mb'],
            'total_revenue': revenue_result['total'],
            'method': 'optimized'
        }
    
    def _benchmark_parallel_method(self, filename):
        """å¹¶è¡Œæ–¹æ³•åŸºå‡†æµ‹è¯•"""
        self.monitor.start_monitoring()
        
        # å¹¶è¡Œå¤„ç†
        results = self.parallel_process_by_region(filename)
        self.monitor.log_operation("å¹¶è¡Œå¤„ç†")
        
        # æ±‡æ€»ç»“æœ
        total_revenue = sum(r['revenue']['total'] for r in results)
        
        performance = self.monitor.stop_monitoring()
        
        return {
            'execution_time': performance['total_time'],
            'memory_used': performance['memory_used_mb'],
            'total_revenue': total_revenue,
            'method': 'parallel'
        }


def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
    """
    print("Session24 æ¼”ç¤ºé¡¹ç›®ï¼šé«˜æ€§èƒ½æ•°æ®å¤„ç†å™¨")
    print("=" * 60)
    
    processor = OptimizedDataProcessor()
    
    try:
        # 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®
        data_file = processor.generate_sample_data(50000)  # 5ä¸‡æ¡è®°å½•
        
        # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_results = processor.benchmark_methods(data_file)
        
        # 3. æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
        print("\n" + "=" * 60)
        print("æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print("=" * 60)
        
        # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        benchmark_results.sort(key=lambda x: x[1]['execution_time'])
        
        print(f"{'æ–¹æ³•':<20} {'æ—¶é—´(ç§’)':<10} {'å†…å­˜(MB)':<10} {'åŠ é€Ÿæ¯”':<8}")
        print("-" * 60)
        
        baseline_time = benchmark_results[-1][1]['execution_time']
        
        for i, (method_name, result) in enumerate(benchmark_results):
            speedup = baseline_time / result['execution_time']
            status = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
            
            print(f"{status} {method_name:<18} "
                  f"{result['execution_time']:<10.4f} "
                  f"{result['memory_used']:<10.2f} "
                  f"{speedup:<8.1f}x")
        
        # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print("\nç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
        analysis_results = processor.parallel_process_by_region(data_file)
        report = processor.generate_report(analysis_results)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = "sales_analysis_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
        
        # 5. æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        cache_info = processor.get_category_multiplier.cache_info()
        print(f"\nç¼“å­˜ç»Ÿè®¡: {cache_info}")
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_info.hits / (cache_info.hits + cache_info.misses):.2%}")
        
        # 6. æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        print("\n=== åˆ†ææŠ¥å‘Šæ‘˜è¦ ===")
        print(f"æ€»æ”¶å…¥: ${report['summary']['total_revenue']:,.2f}")
        print(f"æ€»å®¢æˆ·æ•°: {report['summary']['total_customers']:,}")
        print(f"æ€»è®°å½•æ•°: {report['summary']['total_records']:,}")
        
        print("\nå„åœ°åŒºæ”¶å…¥æ’å:")
        regional_revenues = [(r['region'], r['revenue']['total']) for r in analysis_results]
        regional_revenues.sort(key=lambda x: x[1], reverse=True)
        
        for i, (region, revenue) in enumerate(regional_revenues, 1):
            print(f"  {i}. {region}: ${revenue:,.2f}")
        
        print("\n=== æ€§èƒ½ä¼˜åŒ–è¦ç‚¹æ€»ç»“ ===")
        print("âœ“ ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ–‡ä»¶ï¼Œå‡å°‘å†…å­˜å ç”¨")
        print("âœ“ ä½¿ç”¨namedtupleä¼˜åŒ–æ•°æ®ç»“æ„å†…å­˜")
        print("âœ“ ä½¿ç”¨@lru_cacheç¼“å­˜é‡å¤è®¡ç®—")
        print("âœ“ ä½¿ç”¨defaultdictç®€åŒ–å­—å…¸æ“ä½œ")
        print("âœ“ ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ç‹¬ç«‹ä»»åŠ¡")
        print("âœ“ ä½¿ç”¨æ€§èƒ½ç›‘æ§å™¨è·Ÿè¸ªä¼˜åŒ–æ•ˆæœ")
        
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            Path("sales_data.csv").unlink(missing_ok=True)
        except:
            pass


if __name__ == "__main__":
    main()