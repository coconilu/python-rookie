#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Session24 ç»ƒä¹ é¢˜3ï¼šå¹¶å‘ç¼–ç¨‹å’Œç¼“å­˜ä¼˜åŒ–

é¢˜ç›®æè¿°ï¼š
å®ç°ä¸€ä¸ªç½‘é¡µå†…å®¹ä¸‹è½½å’Œåˆ†æå™¨ï¼Œéœ€è¦ï¼š
1. ä¸‹è½½å¤šä¸ªç½‘é¡µå†…å®¹ï¼ˆæ¨¡æ‹ŸIOå¯†é›†å‹ä»»åŠ¡ï¼‰
2. åˆ†æç½‘é¡µä¸­çš„å…³é”®è¯é¢‘ç‡ï¼ˆæ¨¡æ‹ŸCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
3. ç¼“å­˜åˆ†æç»“æœé¿å…é‡å¤è®¡ç®—

è¦æ±‚æ¯”è¾ƒä»¥ä¸‹å®ç°æ–¹å¼çš„æ€§èƒ½ï¼š
- ä¸²è¡Œå¤„ç† vs å¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹
- æ— ç¼“å­˜ vs æœ‰ç¼“å­˜
- ä¸åŒçš„ç¼“å­˜ç­–ç•¥ï¼ˆLRU, ç®€å•å­—å…¸ç¼“å­˜ï¼‰

æ¨¡æ‹Ÿåœºæ™¯ï¼š
- ç½‘é¡µä¸‹è½½ï¼šæ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿçš„IOæ“ä½œ
- å†…å®¹åˆ†æï¼šè®¡ç®—å¯†é›†å‹çš„æ–‡æœ¬å¤„ç†
- ç¼“å­˜ï¼šé¿å…é‡å¤ä¸‹è½½å’Œåˆ†æç›¸åŒURL

è¾“å‡ºç¤ºä¾‹ï¼š
å„ç§å®ç°æ–¹å¼çš„æ‰§è¡Œæ—¶é—´å’Œç¼“å­˜å‘½ä¸­ç‡å¯¹æ¯”
"""

import time
import random
import threading
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import lru_cache
from collections import defaultdict, OrderedDict
import hashlib
import string


class SimpleCache:
    """
    ç®€å•çš„å­—å…¸ç¼“å­˜å®ç°
    """
    def __init__(self, max_size=128):
        self.cache = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, key):
        if key in self.cache:
            self.hits += 1
            return self.cache[key]
        else:
            self.misses += 1
            return None
    
    def put(self, key, value):
        if len(self.cache) >= self.max_size:
            # ç®€å•çš„æ¸…ç†ç­–ç•¥ï¼šåˆ é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
            first_key = next(iter(self.cache))
            del self.cache[first_key]
        self.cache[key] = value
    
    def get_stats(self):
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate
        }
    
    def clear_stats(self):
        self.hits = 0
        self.misses = 0


class LRUCache:
    """
    LRU (Least Recently Used) ç¼“å­˜å®ç°
    """
    def __init__(self, max_size=128):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, key):
        if key in self.cache:
            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            value = self.cache.pop(key)
            self.cache[key] = value
            self.hits += 1
            return value
        else:
            self.misses += 1
            return None
    
    def put(self, key, value):
        if key in self.cache:
            # æ›´æ–°ç°æœ‰é”®
            self.cache.pop(key)
        elif len(self.cache) >= self.max_size:
            # åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹ï¼ˆç¬¬ä¸€ä¸ªï¼‰
            self.cache.popitem(last=False)
        
        self.cache[key] = value
    
    def get_stats(self):
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate
        }
    
    def clear_stats(self):
        self.hits = 0
        self.misses = 0


def simulate_web_download(url):
    """
    æ¨¡æ‹Ÿç½‘é¡µä¸‹è½½ï¼ˆIOå¯†é›†å‹ä»»åŠ¡ï¼‰
    """
    # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    delay = random.uniform(0.1, 0.5)  # 100-500msçš„éšæœºå»¶è¿Ÿ
    time.sleep(delay)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„ç½‘é¡µå†…å®¹
    content_length = random.randint(1000, 5000)
    words = ['python', 'programming', 'web', 'development', 'code', 'software',
             'computer', 'technology', 'data', 'analysis', 'machine', 'learning',
             'artificial', 'intelligence', 'algorithm', 'performance', 'optimization']
    
    content = []
    for _ in range(content_length // 10):
        content.append(random.choice(words))
    
    return ' '.join(content)


def analyze_content(content):
    """
    åˆ†æç½‘é¡µå†…å®¹ï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
    """
    # æ¨¡æ‹Ÿå¤æ‚çš„æ–‡æœ¬åˆ†æ
    words = content.lower().split()
    
    # è¯é¢‘ç»Ÿè®¡
    word_count = defaultdict(int)
    for word in words:
        # æ¸…ç†æ ‡ç‚¹ç¬¦å·
        clean_word = ''.join(c for c in word if c.isalnum())
        if clean_word:
            word_count[clean_word] += 1
    
    # æ¨¡æ‹Ÿä¸€äº›CPUå¯†é›†å‹è®¡ç®—
    total_chars = sum(len(word) for word in words)
    avg_word_length = total_chars / len(words) if words else 0
    
    # æ‰¾å‡ºæœ€å¸¸è§çš„è¯
    top_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]
    
    # æ·»åŠ ä¸€äº›è®¡ç®—å¯†é›†å‹æ“ä½œ
    hash_value = hashlib.md5(content.encode()).hexdigest()
    
    return {
        'word_count': len(word_count),
        'total_words': len(words),
        'avg_word_length': avg_word_length,
        'top_words': top_words,
        'content_hash': hash_value
    }


def process_url_no_cache(url):
    """
    å¤„ç†å•ä¸ªURLï¼ˆæ— ç¼“å­˜ï¼‰
    """
    content = simulate_web_download(url)
    analysis = analyze_content(content)
    return url, analysis


def process_url_with_cache(url, cache):
    """
    å¤„ç†å•ä¸ªURLï¼ˆå¸¦ç¼“å­˜ï¼‰
    """
    # æ£€æŸ¥ç¼“å­˜
    cached_result = cache.get(url)
    if cached_result is not None:
        return url, cached_result
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®é™…å¤„ç†
    content = simulate_web_download(url)
    analysis = analyze_content(content)
    
    # å­˜å…¥ç¼“å­˜
    cache.put(url, analysis)
    
    return url, analysis


# ä¸²è¡Œå¤„ç†
def process_urls_serial(urls, use_cache=False, cache=None):
    """
    ä¸²è¡Œå¤„ç†URLåˆ—è¡¨
    """
    results = []
    
    for url in urls:
        if use_cache and cache:
            result = process_url_with_cache(url, cache)
        else:
            result = process_url_no_cache(url)
        results.append(result)
    
    return results


# å¤šçº¿ç¨‹å¤„ç†
def process_urls_threaded(urls, max_workers=4, use_cache=False, cache=None):
    """
    å¤šçº¿ç¨‹å¤„ç†URLåˆ—è¡¨ï¼ˆé€‚åˆIOå¯†é›†å‹ï¼‰
    """
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        if use_cache and cache:
            futures = [executor.submit(process_url_with_cache, url, cache) for url in urls]
        else:
            futures = [executor.submit(process_url_no_cache, url) for url in urls]
        
        for future in futures:
            results.append(future.result())
    
    return results


# å¤šè¿›ç¨‹å¤„ç†
def process_urls_multiprocess(urls, max_workers=4):
    """
    å¤šè¿›ç¨‹å¤„ç†URLåˆ—è¡¨ï¼ˆé€‚åˆCPUå¯†é›†å‹ï¼‰
    æ³¨æ„ï¼šå¤šè¿›ç¨‹ä¸èƒ½ç›´æ¥å…±äº«ç¼“å­˜å¯¹è±¡
    """
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_url_no_cache, url) for url in urls]
        results = [future.result() for future in futures]
    
    return results


def generate_test_urls(num_urls=50, duplicate_rate=0.3):
    """
    ç”Ÿæˆæµ‹è¯•URLåˆ—è¡¨
    duplicate_rate: é‡å¤URLçš„æ¯”ä¾‹ï¼Œç”¨äºæµ‹è¯•ç¼“å­˜æ•ˆæœ
    """
    base_urls = [f"https://example{i}.com/page{j}" 
                 for i in range(1, 21) for j in range(1, 6)]  # 100ä¸ªåŸºç¡€URL
    
    urls = []
    num_duplicates = int(num_urls * duplicate_rate)
    num_unique = num_urls - num_duplicates
    
    # æ·»åŠ å”¯ä¸€URL
    unique_urls = random.sample(base_urls, min(num_unique, len(base_urls)))
    urls.extend(unique_urls)
    
    # æ·»åŠ é‡å¤URL
    for _ in range(num_duplicates):
        urls.append(random.choice(unique_urls))
    
    # æ‰“ä¹±é¡ºåº
    random.shuffle(urls)
    
    return urls


def benchmark_method(method_name, method_func, urls, **kwargs):
    """
    åŸºå‡†æµ‹è¯•æ–¹æ³•
    """
    print(f"\næµ‹è¯• {method_name}...")
    
    start_time = time.time()
    results = method_func(urls, **kwargs)
    end_time = time.time()
    
    execution_time = end_time - start_time
    
    print(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.4f} ç§’")
    print(f"  å¤„ç†URLæ•°: {len(results)}")
    print(f"  å¹³å‡æ¯URL: {execution_time/len(results):.4f} ç§’")
    
    # å¦‚æœä½¿ç”¨äº†ç¼“å­˜ï¼Œæ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    if 'cache' in kwargs and kwargs['cache']:
        cache_stats = kwargs['cache'].get_stats()
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}")
        print(f"  ç¼“å­˜å‘½ä¸­: {cache_stats['hits']}, æœªå‘½ä¸­: {cache_stats['misses']}")
    
    return {
        'method': method_name,
        'time': execution_time,
        'results': results,
        'cache_stats': kwargs['cache'].get_stats() if 'cache' in kwargs and kwargs['cache'] else None
    }


def performance_comparison():
    """
    æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    """
    print("Session24 ç»ƒä¹ é¢˜3: å¹¶å‘ç¼–ç¨‹å’Œç¼“å­˜ä¼˜åŒ–")
    print("=" * 60)
    
    # ç”Ÿæˆæµ‹è¯•URL
    test_urls = generate_test_urls(30, duplicate_rate=0.4)  # 30ä¸ªURLï¼Œ40%é‡å¤ç‡
    unique_urls = len(set(test_urls))
    
    print(f"ç”Ÿæˆæµ‹è¯•URL: {len(test_urls)} ä¸ª (å…¶ä¸­ {unique_urls} ä¸ªå”¯ä¸€)")
    print(f"é‡å¤ç‡: {(len(test_urls) - unique_urls) / len(test_urls):.1%}")
    
    # å‡†å¤‡ä¸åŒçš„ç¼“å­˜
    simple_cache = SimpleCache(max_size=50)
    lru_cache = LRUCache(max_size=50)
    
    # æµ‹è¯•æ–¹æ³•åˆ—è¡¨
    test_methods = [
        ("ä¸²è¡Œå¤„ç†(æ— ç¼“å­˜)", process_urls_serial, {'use_cache': False}),
        ("ä¸²è¡Œå¤„ç†(ç®€å•ç¼“å­˜)", process_urls_serial, {'use_cache': True, 'cache': simple_cache}),
        ("ä¸²è¡Œå¤„ç†(LRUç¼“å­˜)", process_urls_serial, {'use_cache': True, 'cache': lru_cache}),
        ("å¤šçº¿ç¨‹(æ— ç¼“å­˜)", process_urls_threaded, {'max_workers': 4, 'use_cache': False}),
        ("å¤šçº¿ç¨‹(ç®€å•ç¼“å­˜)", process_urls_threaded, {'max_workers': 4, 'use_cache': True, 'cache': SimpleCache(50)}),
        ("å¤šè¿›ç¨‹(æ— ç¼“å­˜)", process_urls_multiprocess, {'max_workers': 4}),
    ]
    
    results = []
    
    for method_name, method_func, kwargs in test_methods:
        try:
            # æ¸…ç†ç¼“å­˜ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'cache' in kwargs and kwargs['cache']:
                kwargs['cache'].clear_stats()
            
            result = benchmark_method(method_name, method_func, test_urls, **kwargs)
            results.append(result)
            
        except Exception as e:
            print(f"  âŒ æ–¹æ³• {method_name} æ‰§è¡Œå¤±è´¥: {e}")
    
    # ç»“æœå¯¹æ¯”
    print("\n" + "=" * 70)
    print("æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 70)
    
    # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
    results.sort(key=lambda x: x['time'])
    
    print(f"{'æ–¹æ³•':<20} {'æ—¶é—´(ç§’)':<10} {'åŠ é€Ÿæ¯”':<8} {'ç¼“å­˜å‘½ä¸­ç‡':<12}")
    print("-" * 70)
    
    baseline_time = results[-1]['time']  # æœ€æ…¢çš„ä½œä¸ºåŸºå‡†
    
    for i, result in enumerate(results):
        speedup = baseline_time / result['time']
        cache_hit_rate = ""
        
        if result['cache_stats']:
            cache_hit_rate = f"{result['cache_stats']['hit_rate']:.1%}"
        else:
            cache_hit_rate = "N/A"
        
        status = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
        
        print(f"{status} {result['method']:<18} "
              f"{result['time']:<10.4f} "
              f"{speedup:<8.1f}x "
              f"{cache_hit_rate:<12}")
    
    # æ˜¾ç¤ºåˆ†æç»“æœç¤ºä¾‹
    if results:
        sample_result = results[0]['results'][0][1]  # ç¬¬ä¸€ä¸ªURLçš„åˆ†æç»“æœ
        print(f"\nåˆ†æç»“æœç¤ºä¾‹:")
        print(f"æ€»è¯æ•°: {sample_result['total_words']}")
        print(f"å”¯ä¸€è¯æ•°: {sample_result['word_count']}")
        print(f"å¹³å‡è¯é•¿: {sample_result['avg_word_length']:.2f}")
        print(f"é«˜é¢‘è¯: {sample_result['top_words'][:3]}")


@lru_cache(maxsize=128)
def cached_fibonacci(n):
    """
    ä½¿ç”¨å†…ç½®lru_cacheçš„æ–æ³¢é‚£å¥‘æ•°åˆ—
    """
    if n < 2:
        return n
    return cached_fibonacci(n-1) + cached_fibonacci(n-2)


def demo_builtin_cache():
    """
    æ¼”ç¤ºPythonå†…ç½®çš„lru_cacheè£…é¥°å™¨
    """
    print("\n=== å†…ç½®lru_cacheæ¼”ç¤º ===")
    
    # æµ‹è¯•ç¼“å­˜æ•ˆæœ
    start_time = time.time()
    result = cached_fibonacci(35)
    end_time = time.time()
    
    print(f"fibonacci(35) = {result}")
    print(f"æ‰§è¡Œæ—¶é—´: {end_time - start_time:.6f} ç§’")
    print(f"ç¼“å­˜ä¿¡æ¯: {cached_fibonacci.cache_info()}")
    
    # å†æ¬¡è®¡ç®—ï¼Œåº”è¯¥å¾ˆå¿«
    start_time = time.time()
    result = cached_fibonacci(35)
    end_time = time.time()
    
    print(f"å†æ¬¡è®¡ç®— fibonacci(35) = {result}")
    print(f"æ‰§è¡Œæ—¶é—´: {end_time - start_time:.6f} ç§’")
    print(f"ç¼“å­˜ä¿¡æ¯: {cached_fibonacci.cache_info()}")


def solution():
    """
    ç»ƒä¹ è§£å†³æ–¹æ¡ˆ
    """
    try:
        performance_comparison()
        demo_builtin_cache()
        
        print("\n=== å­¦ä¹ è¦ç‚¹ ===")
        print("1. å¤šçº¿ç¨‹é€‚åˆIOå¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ï¼‰")
        print("2. å¤šè¿›ç¨‹é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å¤æ‚è®¡ç®—ï¼‰")
        print("3. ç¼“å­˜å¯ä»¥æ˜¾è‘—æé«˜é‡å¤æ“ä½œçš„æ€§èƒ½")
        print("4. LRUç¼“å­˜åœ¨å†…å­˜æœ‰é™æ—¶æ¯”ç®€å•ç¼“å­˜æ›´æ™ºèƒ½")
        print("5. Pythonçš„@lru_cacheè£…é¥°å™¨ä½¿ç”¨ç®€å•ä¸”é«˜æ•ˆ")
        print("6. å¹¶å‘ç¼–ç¨‹éœ€è¦è€ƒè™‘çº¿ç¨‹å®‰å…¨å’Œæ•°æ®å…±äº«é—®é¢˜")
        
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    solution()