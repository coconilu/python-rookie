"""
ç»ƒä¹  14.3ï¼šç»¼åˆç»ƒä¹  - æ–°é—»èšåˆå™¨

ä»»åŠ¡æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªç®€å•çš„æ–°é—»èšåˆå™¨ï¼Œä»å¤šä¸ªæ¥æºè·å–æ–°é—»ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç»¼åˆæŠ¥å‘Šã€‚

è¦æ±‚ï¼š
1. ä»è‡³å°‘2ä¸ªä¸åŒçš„APIè·å–æ–°é—»æˆ–æ–‡ç« ï¼š
   - https://hacker-news.firebaseio.com/v0/topstories.json (Hacker News)
   - https://api.github.com/events (GitHubå…¬å¼€äº‹ä»¶)
   
2. å¯¹äºHacker News:
   - è·å–å‰10ä¸ªçƒ­é—¨æ•…äº‹ID
   - è·å–æ¯ä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€URLã€è¯„åˆ†ç­‰ï¼‰
   
3. å¯¹äºGitHub Events:
   - è·å–æœ€æ–°çš„10ä¸ªå…¬å¼€äº‹ä»¶
   - æå–äº‹ä»¶ç±»å‹ã€ä»“åº“åç§°ã€ç”¨æˆ·ç­‰ä¿¡æ¯
   
4. å®ç°ä¸€ä¸ªç®€å•çš„ç½‘é¡µçˆ¬è™«ï¼Œä»ä¸€ä¸ªæ–°é—»ç½‘ç«™æå–æ ‡é¢˜
   
5. å°†æ‰€æœ‰ä¿¡æ¯æ•´åˆæˆä¸€ä¸ªæ ¼å¼åŒ–çš„æŠ¥å‘Š

6. é«˜çº§åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰ï¼š
   - å®ç°å…³é”®è¯è¿‡æ»¤
   - æŒ‰æ—¶é—´æ’åº
   - ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š
   - æ·»åŠ ç®€å•çš„æƒ…æ„Ÿåˆ†æ

ç¤ºä¾‹è¾“å‡ºï¼š
=== ä»Šæ—¥ç§‘æŠ€æ–°é—»èšåˆ ===
ç”Ÿæˆæ—¶é—´: 2024-01-15 14:30:00

ã€Hacker News çƒ­é—¨ã€‘
1. â­ 1234 - "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿"
   é“¾æ¥: https://example.com/ai-future
   è¯„è®º: 456æ¡

2. â­ 987 - "Python 3.13 æ–°ç‰¹æ€§é¢„è§ˆ"
   é“¾æ¥: https://example.com/python-313
   è¯„è®º: 234æ¡

ã€GitHub åŠ¨æ€ã€‘
1. ğŸ”€ facebook/react - Pull Request merged by user123
2. â­ microsoft/vscode - Starred by user456
3. ğŸ´ python/cpython - Forked by user789

ã€å…¶ä»–æ–°é—»ã€‘
- "æŠ€æœ¯å…¬å¸å­£åº¦è´¢æŠ¥è¶…é¢„æœŸ"
- "æ–°çš„å¼€æºé¡¹ç›®è·å¾—å¤§é‡å…³æ³¨"

æ€»è®¡: 22æ¡æ–°é—»
çƒ­é—¨å…³é”®è¯: AI, Python, å¼€æº

æç¤ºï¼š
- Hacker News APIæ–‡æ¡£: https://github.com/HackerNews/API
- å•ä¸ªæ•…äº‹çš„API: https://hacker-news.firebaseio.com/v0/item/{id}.json
- æ³¨æ„å¤„ç†APIé™æµ
- ä½¿ç”¨åˆé€‚çš„æ•°æ®ç»“æ„å­˜å‚¨æ–°é—»
"""

import json
import time
import re
from datetime import datetime
from collections import Counter

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    import urllib.request
    import urllib.error
    HAS_REQUESTS = False


class NewsAggregator:
    """æ–°é—»èšåˆå™¨ç±»"""
    
    def __init__(self):
        self.hn_api = "https://hacker-news.firebaseio.com/v0"
        self.github_api = "https://api.github.com"
        self.news_data = {
            'hackernews': [],
            'github': [],
            'crawled': []
        }
        self.keywords = []
    
    def fetch_hackernews_top_stories(self, limit=10):
        """
        è·å–Hacker Newsçƒ­é—¨æ•…äº‹
        
        å‚æ•°:
            limit: è·å–çš„æ•…äº‹æ•°é‡
        
        è¿”å›:
            list: æ•…äº‹è¯¦æƒ…åˆ—è¡¨
        """
        # TODO: å®ç°è·å–Hacker Newsçƒ­é—¨æ•…äº‹
        # æ­¥éª¤ï¼š
        # 1. è·å–çƒ­é—¨æ•…äº‹IDåˆ—è¡¨: /topstories.json
        # 2. è·å–å‰limitä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯
        # 3. æå–æ ‡é¢˜ã€URLã€è¯„åˆ†ã€è¯„è®ºæ•°ç­‰
        # 4. å¤„ç†å¯èƒ½çš„é”™è¯¯
        
        pass
    
    def fetch_github_events(self, limit=10):
        """
        è·å–GitHubå…¬å¼€äº‹ä»¶
        
        å‚æ•°:
            limit: è·å–çš„äº‹ä»¶æ•°é‡
        
        è¿”å›:
            list: äº‹ä»¶åˆ—è¡¨
        """
        # TODO: å®ç°è·å–GitHubäº‹ä»¶
        # æ­¥éª¤ï¼š
        # 1. è°ƒç”¨ /events API
        # 2. è§£æäº‹ä»¶ç±»å‹ã€ä»“åº“ã€ç”¨æˆ·ç­‰ä¿¡æ¯
        # 3. æ ¼å¼åŒ–äº‹ä»¶æè¿°
        
        pass
    
    def crawl_news_website(self, url):
        """
        ä»æŒ‡å®šç½‘ç«™çˆ¬å–æ–°é—»æ ‡é¢˜
        
        å‚æ•°:
            url: è¦çˆ¬å–çš„ç½‘ç«™URL
        
        è¿”å›:
            list: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        """
        # TODO: å®ç°ç®€å•çš„æ–°é—»çˆ¬è™«
        # æ­¥éª¤ï¼š
        # 1. è·å–ç½‘é¡µå†…å®¹
        # 2. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ ‡é¢˜
        # 3. æ¸…ç†å’Œæ ¼å¼åŒ–æ ‡é¢˜
        # 4. æ³¨æ„çˆ¬è™«ç¤¼ä»ª
        
        pass
    
    def filter_by_keywords(self, items, keywords):
        """
        æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–°é—»
        
        å‚æ•°:
            items: æ–°é—»é¡¹ç›®åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
        
        è¿”å›:
            list: è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        # TODO: å®ç°å…³é”®è¯è¿‡æ»¤
        # å¯ä»¥æ£€æŸ¥æ ‡é¢˜æˆ–å†…å®¹ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        
        pass
    
    def extract_keywords(self, texts, top_n=5):
        """
        ä»æ–‡æœ¬ä¸­æå–çƒ­é—¨å…³é”®è¯
        
        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
        
        è¿”å›:
            list: å…³é”®è¯åˆ—è¡¨
        """
        # TODO: å®ç°ç®€å•çš„å…³é”®è¯æå–
        # æ­¥éª¤ï¼š
        # 1. åˆ†è¯ï¼ˆç®€å•çš„æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        # 2. è¿‡æ»¤å¸¸è§è¯ï¼ˆthe, a, isç­‰ï¼‰
        # 3. ç»Ÿè®¡è¯é¢‘
        # 4. è¿”å›é«˜é¢‘è¯
        
        pass
    
    def format_report(self):
        """
        ç”Ÿæˆæ ¼å¼åŒ–çš„æ–°é—»æŠ¥å‘Š
        
        è¿”å›:
            str: æ ¼å¼åŒ–çš„æŠ¥å‘Š
        """
        # TODO: å®ç°æŠ¥å‘Šç”Ÿæˆ
        # åŒ…å«ï¼š
        # 1. æ—¶é—´æˆ³
        # 2. å„ä¸ªæ¥æºçš„æ–°é—»
        # 3. ç»Ÿè®¡ä¿¡æ¯
        # 4. çƒ­é—¨å…³é”®è¯
        
        pass
    
    def generate_html_report(self):
        """
        ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Šï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        
        è¿”å›:
            str: HTMLæ ¼å¼çš„æŠ¥å‘Š
        """
        # TODO: ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
        # å¯ä»¥åŒ…å«ï¼š
        # 1. CSSæ ·å¼
        # 2. å¯ç‚¹å‡»çš„é“¾æ¥
        # 3. æ›´å¥½çš„æ ¼å¼åŒ–
        
        pass
    
    def analyze_sentiment(self, text):
        """
        ç®€å•çš„æƒ…æ„Ÿåˆ†æï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        
        å‚æ•°:
            text: è¦åˆ†æçš„æ–‡æœ¬
        
        è¿”å›:
            str: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
        """
        # TODO: å®ç°ç®€å•çš„æƒ…æ„Ÿåˆ†æï¼ˆå¯é€‰ï¼‰
        # å¯ä»¥åŸºäºå…³é”®è¯åŒ¹é…
        
        pass


def test_api_connection():
    """æµ‹è¯•APIè¿æ¥"""
    print("æµ‹è¯•APIè¿æ¥...")
    
    # æµ‹è¯•Hacker News API
    try:
        if HAS_REQUESTS:
            response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json")
            success = response.status_code == 200
        else:
            response = urllib.request.urlopen("https://hacker-news.firebaseio.com/v0/topstories.json")
            success = response.status == 200
        
        print(f"Hacker News API: {'âœ“ è¿æ¥æˆåŠŸ' if success else 'âœ— è¿æ¥å¤±è´¥'}")
    except Exception as e:
        print(f"Hacker News API: âœ— è¿æ¥å¤±è´¥ - {e}")
    
    # æµ‹è¯•GitHub API
    try:
        if HAS_REQUESTS:
            response = requests.get("https://api.github.com/events")
            success = response.status_code == 200
        else:
            response = urllib.request.urlopen("https://api.github.com/events")
            success = response.status == 200
        
        print(f"GitHub API: {'âœ“ è¿æ¥æˆåŠŸ' if success else 'âœ— è¿æ¥å¤±è´¥'}")
    except Exception as e:
        print(f"GitHub API: âœ— è¿æ¥å¤±è´¥ - {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ç»ƒä¹ 14.3ï¼šæ–°é—»èšåˆå™¨\n")
    
    # æµ‹è¯•è¿æ¥
    test_api_connection()
    print()
    
    # åˆ›å»ºèšåˆå™¨
    aggregator = NewsAggregator()
    
    # è®¾ç½®å…³é”®è¯è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    # aggregator.keywords = ['Python', 'AI', 'å¼€æº']
    
    print("æ­£åœ¨è·å–æ–°é—»...")
    
    # è·å–Hacker News
    print("- è·å–Hacker Newsçƒ­é—¨æ•…äº‹...")
    hn_stories = aggregator.fetch_hackernews_top_stories(5)
    
    # è·å–GitHubäº‹ä»¶
    print("- è·å–GitHubå…¬å¼€äº‹ä»¶...")
    github_events = aggregator.fetch_github_events(5)
    
    # çˆ¬å–æ–°é—»ç½‘ç«™ï¼ˆä½¿ç”¨httpbinä½œä¸ºç¤ºä¾‹ï¼‰
    print("- çˆ¬å–æ–°é—»ç½‘ç«™...")
    crawled_news = aggregator.crawl_news_website("http://httpbin.org/html")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nç”ŸæˆæŠ¥å‘Š...\n")
    report = aggregator.format_report()
    
    if report:
        print(report)
        
        # å¯é€‰ï¼šç”ŸæˆHTMLæŠ¥å‘Š
        # html_report = aggregator.generate_html_report()
        # with open('news_report.html', 'w', encoding='utf-8') as f:
        #     f.write(html_report)
        # print("\nHTMLæŠ¥å‘Šå·²ä¿å­˜åˆ° news_report.html")
    else:
        print("ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼")


# è¾…åŠ©å‡½æ•°
def format_number(num):
    """æ ¼å¼åŒ–å¤§æ•°å­—æ˜¾ç¤º"""
    if num >= 1000:
        return f"{num/1000:.1f}k"
    return str(num)


def clean_html(text):
    """æ¸…ç†HTMLæ ‡ç­¾"""
    return re.sub(r'<[^>]+>', '', text)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    main()
    
    # å•å…ƒæµ‹è¯•
    print("\n" + "="*50)
    print("å•å…ƒæµ‹è¯•ï¼š")
    
    aggregator = NewsAggregator()
    
    # æµ‹è¯•å…³é”®è¯æå–
    print("\næµ‹è¯•å…³é”®è¯æå–:")
    sample_texts = [
        "Python programming is awesome",
        "Machine learning with Python",
        "Python for data science"
    ]
    keywords = aggregator.extract_keywords(sample_texts)
    print(f"æå–çš„å…³é”®è¯: {keywords}")
    
    # æµ‹è¯•æ•°å­—æ ¼å¼åŒ–
    print("\næµ‹è¯•æ•°å­—æ ¼å¼åŒ–:")
    test_numbers = [123, 1234, 12345, 123456]
    for num in test_numbers:
        print(f"{num} -> {format_number(num)}") 