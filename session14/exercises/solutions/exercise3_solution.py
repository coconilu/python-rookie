"""
ç»ƒä¹  14.3 å‚è€ƒç­”æ¡ˆï¼šç»¼åˆç»ƒä¹  - æ–°é—»èšåˆå™¨
"""

import json
import time
import re
from datetime import datetime
from collections import Counter

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    import urllib.request
    import urllib.error
    HAS_REQUESTS = False


class NewsAggregator:
    """æ–°é—»èšåˆå™¨ç±»"""
    
    def __init__(self):
        self.hn_api = "https://hacker-news.firebaseio.com/v0"
        self.github_api = "https://api.github.com"
        self.news_data = {
            'hackernews': [],
            'github': [],
            'crawled': []
        }
        self.keywords = []
        # å¸¸è§çš„åœç”¨è¯
        self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                              'to', 'for', 'of', 'with', 'by', 'from', 'is', 'was', 
                              'are', 'were', 'been', 'be', 'have', 'has', 'had'])
    
    def fetch_hackernews_top_stories(self, limit=10):
        """
        è·å–Hacker Newsçƒ­é—¨æ•…äº‹
        
        å‚æ•°:
            limit: è·å–çš„æ•…äº‹æ•°é‡
        
        è¿”å›:
            list: æ•…äº‹è¯¦æƒ…åˆ—è¡¨
        """
        stories = []
        
        try:
            # è·å–çƒ­é—¨æ•…äº‹IDåˆ—è¡¨
            url = f"{self.hn_api}/topstories.json"
            
            if HAS_REQUESTS:
                response = requests.get(url, timeout=10)
                story_ids = response.json()
            else:
                response = urllib.request.urlopen(url, timeout=10)
                story_ids = json.loads(response.read().decode('utf-8'))
            
            # è·å–å‰limitä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯
            for story_id in story_ids[:limit]:
                story_url = f"{self.hn_api}/item/{story_id}.json"
                
                try:
                    if HAS_REQUESTS:
                        response = requests.get(story_url, timeout=5)
                        story_data = response.json()
                    else:
                        response = urllib.request.urlopen(story_url, timeout=5)
                        story_data = json.loads(response.read().decode('utf-8'))
                    
                    # æå–éœ€è¦çš„ä¿¡æ¯
                    story = {
                        'title': story_data.get('title', 'æ— æ ‡é¢˜'),
                        'url': story_data.get('url', ''),
                        'score': story_data.get('score', 0),
                        'by': story_data.get('by', 'anonymous'),
                        'time': story_data.get('time', 0),
                        'descendants': story_data.get('descendants', 0)  # è¯„è®ºæ•°
                    }
                    stories.append(story)
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.2)
                    
                except Exception as e:
                    print(f"è·å–æ•…äº‹ {story_id} å¤±è´¥: {e}")
            
            self.news_data['hackernews'] = stories
            
        except Exception as e:
            print(f"è·å–Hacker Newsçƒ­é—¨æ•…äº‹å¤±è´¥: {e}")
        
        return stories
    
    def fetch_github_events(self, limit=10):
        """
        è·å–GitHubå…¬å¼€äº‹ä»¶
        
        å‚æ•°:
            limit: è·å–çš„äº‹ä»¶æ•°é‡
        
        è¿”å›:
            list: äº‹ä»¶åˆ—è¡¨
        """
        events = []
        
        try:
            url = f"{self.github_api}/events"
            
            if HAS_REQUESTS:
                response = requests.get(url, timeout=10)
                all_events = response.json()
            else:
                request = urllib.request.Request(url)
                request.add_header('Accept', 'application/vnd.github.v3+json')
                response = urllib.request.urlopen(request, timeout=10)
                all_events = json.loads(response.read().decode('utf-8'))
            
            # å¤„ç†äº‹ä»¶æ•°æ®
            for event_data in all_events[:limit]:
                event_type = event_data.get('type', 'Unknown')
                actor = event_data.get('actor', {}).get('login', 'unknown')
                repo = event_data.get('repo', {}).get('name', 'unknown/unknown')
                created_at = event_data.get('created_at', '')
                
                # æ ¹æ®äº‹ä»¶ç±»å‹ç”Ÿæˆæè¿°
                event_icons = {
                    'PushEvent': 'ğŸ“¤',
                    'PullRequestEvent': 'ğŸ”€',
                    'IssuesEvent': 'ğŸ“',
                    'WatchEvent': 'â­',
                    'ForkEvent': 'ğŸ´',
                    'CreateEvent': 'ğŸ†•',
                    'DeleteEvent': 'ğŸ—‘ï¸'
                }
                
                icon = event_icons.get(event_type, 'ğŸ“Œ')
                
                event = {
                    'type': event_type,
                    'actor': actor,
                    'repo': repo,
                    'created_at': created_at,
                    'icon': icon,
                    'description': f"{icon} {repo} - {event_type.replace('Event', '')} by {actor}"
                }
                
                events.append(event)
            
            self.news_data['github'] = events
            
        except Exception as e:
            print(f"è·å–GitHubäº‹ä»¶å¤±è´¥: {e}")
        
        return events
    
    def crawl_news_website(self, url):
        """
        ä»æŒ‡å®šç½‘ç«™çˆ¬å–æ–°é—»æ ‡é¢˜
        
        å‚æ•°:
            url: è¦çˆ¬å–çš„ç½‘ç«™URL
        
        è¿”å›:
            list: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        """
        titles = []
        
        try:
            # è®¾ç½®User-Agent
            if HAS_REQUESTS:
                headers = {'User-Agent': 'Python News Aggregator 1.0'}
                response = requests.get(url, headers=headers, timeout=10)
                html = response.text
            else:
                request = urllib.request.Request(url)
                request.add_header('User-Agent', 'Python News Aggregator 1.0')
                response = urllib.request.urlopen(request, timeout=10)
                html = response.read().decode('utf-8')
            
            # æå–æ‰€æœ‰æ ‡é¢˜æ ‡ç­¾å†…å®¹
            title_patterns = [
                r'<h1[^>]*>(.*?)</h1>',
                r'<h2[^>]*>(.*?)</h2>',
                r'<h3[^>]*>(.*?)</h3>',
                r'<title>(.*?)</title>'
            ]
            
            for pattern in title_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    # æ¸…ç†HTMLæ ‡ç­¾å’Œç©ºç™½
                    clean_title = re.sub(r'<[^>]+>', '', match)
                    clean_title = clean_title.strip()
                    if clean_title and len(clean_title) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                        titles.append(clean_title)
            
            # å»é‡
            titles = list(set(titles))
            self.news_data['crawled'] = titles[:10]  # é™åˆ¶æ•°é‡
            
        except Exception as e:
            print(f"çˆ¬å–ç½‘ç«™å¤±è´¥: {e}")
        
        return titles
    
    def filter_by_keywords(self, items, keywords):
        """
        æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–°é—»
        
        å‚æ•°:
            items: æ–°é—»é¡¹ç›®åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
        
        è¿”å›:
            list: è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        if not keywords:
            return items
        
        filtered = []
        keywords_lower = [k.lower() for k in keywords]
        
        for item in items:
            # è·å–è¦æ£€æŸ¥çš„æ–‡æœ¬
            if isinstance(item, dict):
                text = item.get('title', '') + ' ' + item.get('description', '')
            else:
                text = str(item)
            
            text_lower = text.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯
            if any(keyword in text_lower for keyword in keywords_lower):
                filtered.append(item)
        
        return filtered
    
    def extract_keywords(self, texts, top_n=5):
        """
        ä»æ–‡æœ¬ä¸­æå–çƒ­é—¨å…³é”®è¯
        
        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
        
        è¿”å›:
            list: å…³é”®è¯åˆ—è¡¨
        """
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ' '.join(texts)
        
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        words = re.findall(r'\b[a-zA-Z]+\b', all_text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [w for w in words 
                         if w not in self.stop_words and len(w) > 3]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        # è¿”å›æœ€å¸¸è§çš„è¯
        return [word for word, count in word_counts.most_common(top_n)]
    
    def format_report(self):
        """
        ç”Ÿæˆæ ¼å¼åŒ–çš„æ–°é—»æŠ¥å‘Š
        
        è¿”å›:
            str: æ ¼å¼åŒ–çš„æŠ¥å‘Š
        """
        report = []
        total_news = 0
        
        # æŠ¥å‘Šå¤´éƒ¨
        report.append("=" * 50)
        report.append("=== ä»Šæ—¥ç§‘æŠ€æ–°é—»èšåˆ ===")
        report.append("=" * 50)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Hacker Newséƒ¨åˆ†
        if self.news_data['hackernews']:
            report.append("ã€Hacker News çƒ­é—¨ã€‘")
            for i, story in enumerate(self.news_data['hackernews'][:5], 1):
                report.append(f"{i}. â­ {story['score']} - \"{story['title']}\"")
                if story['url']:
                    report.append(f"   é“¾æ¥: {story['url']}")
                report.append(f"   è¯„è®º: {story['descendants']}æ¡")
                report.append("")
            total_news += len(self.news_data['hackernews'])
        
        # GitHubäº‹ä»¶éƒ¨åˆ†
        if self.news_data['github']:
            report.append("ã€GitHub åŠ¨æ€ã€‘")
            for i, event in enumerate(self.news_data['github'][:5], 1):
                report.append(f"{i}. {event['description']}")
            report.append("")
            total_news += len(self.news_data['github'])
        
        # çˆ¬å–çš„æ–°é—»
        if self.news_data['crawled']:
            report.append("ã€å…¶ä»–æ–°é—»ã€‘")
            for title in self.news_data['crawled'][:5]:
                report.append(f"- \"{title}\"")
            report.append("")
            total_news += len(self.news_data['crawled'])
        
        # ç»Ÿè®¡ä¿¡æ¯
        report.append("=" * 50)
        report.append(f"æ€»è®¡: {total_news}æ¡æ–°é—»")
        
        # æå–å…³é”®è¯
        all_texts = []
        for story in self.news_data['hackernews']:
            all_texts.append(story['title'])
        all_texts.extend(self.news_data['crawled'])
        
        if all_texts:
            keywords = self.extract_keywords(all_texts, 10)
            report.append(f"çƒ­é—¨å…³é”®è¯: {', '.join(keywords)}")
        
        report.append("=" * 50)
        
        return "\n".join(report)
    
    def generate_html_report(self):
        """
        ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Šï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        
        è¿”å›:
            str: HTMLæ ¼å¼çš„æŠ¥å‘Š
        """
        html = []
        html.append("""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>ç§‘æŠ€æ–°é—»èšåˆ</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
                h1 { color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px; }
                h2 { color: #007bff; margin-top: 30px; }
                .news-item { background: white; padding: 15px; margin: 10px 0; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
                .score { color: #ff6600; font-weight: bold; }
                .link { color: #007bff; text-decoration: none; }
                .link:hover { text-decoration: underline; }
                .stats { color: #666; font-size: 0.9em; }
                .keywords { background: #e9ecef; padding: 10px; border-radius: 5px; margin-top: 20px; }
            </style>
        </head>
        <body>
        """)
        
        html.append(f"<h1>ç§‘æŠ€æ–°é—»èšåˆ - {datetime.now().strftime('%Y-%m-%d')}</h1>")
        
        # Hacker News
        if self.news_data['hackernews']:
            html.append("<h2>Hacker News çƒ­é—¨</h2>")
            for story in self.news_data['hackernews'][:10]:
                html.append('<div class="news-item">')
                html.append(f'<span class="score">â­ {story["score"]}</span> ')
                html.append(f'<strong>{story["title"]}</strong><br>')
                if story['url']:
                    html.append(f'<a class="link" href="{story["url"]}" target="_blank">æŸ¥çœ‹åŸæ–‡</a> | ')
                html.append(f'<span class="stats">è¯„è®º: {story["descendants"]}æ¡</span>')
                html.append('</div>')
        
        # GitHubäº‹ä»¶
        if self.news_data['github']:
            html.append("<h2>GitHub åŠ¨æ€</h2>")
            for event in self.news_data['github'][:10]:
                html.append('<div class="news-item">')
                html.append(f'{event["description"]}')
                html.append('</div>')
        
        html.append("</body></html>")
        
        return "\n".join(html)
    
    def analyze_sentiment(self, text):
        """
        ç®€å•çš„æƒ…æ„Ÿåˆ†æï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        
        å‚æ•°:
            text: è¦åˆ†æçš„æ–‡æœ¬
        
        è¿”å›:
            str: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
        """
        # ç®€å•çš„åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 
                         'fantastic', 'love', 'best', 'awesome', 'brilliant']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 
                         'hate', 'poor', 'disappointing', 'failure', 'broken']
        
        text_lower = text.lower()
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return "æ­£é¢ ğŸ˜Š"
        elif negative_count > positive_count:
            return "è´Ÿé¢ ğŸ˜"
        else:
            return "ä¸­æ€§ ğŸ˜"


def test_api_connection():
    """æµ‹è¯•APIè¿æ¥"""
    print("æµ‹è¯•APIè¿æ¥...")
    
    # æµ‹è¯•Hacker News API
    try:
        if HAS_REQUESTS:
            response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=5)
            success = response.status_code == 200
        else:
            response = urllib.request.urlopen("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=5)
            success = response.status == 200
        
        print(f"Hacker News API: {'âœ“ è¿æ¥æˆåŠŸ' if success else 'âœ— è¿æ¥å¤±è´¥'}")
    except Exception as e:
        print(f"Hacker News API: âœ— è¿æ¥å¤±è´¥ - {e}")
    
    # æµ‹è¯•GitHub API
    try:
        if HAS_REQUESTS:
            response = requests.get("https://api.github.com/events", timeout=5)
            success = response.status_code == 200
        else:
            response = urllib.request.urlopen("https://api.github.com/events", timeout=5)
            success = response.status == 200
        
        print(f"GitHub API: {'âœ“ è¿æ¥æˆåŠŸ' if success else 'âœ— è¿æ¥å¤±è´¥'}")
    except Exception as e:
        print(f"GitHub API: âœ— è¿æ¥å¤±è´¥ - {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ç»ƒä¹ 14.3ï¼šæ–°é—»èšåˆå™¨\n")
    
    # æµ‹è¯•è¿æ¥
    test_api_connection()
    print()
    
    # åˆ›å»ºèšåˆå™¨
    aggregator = NewsAggregator()
    
    # è®¾ç½®å…³é”®è¯è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    # aggregator.keywords = ['Python', 'AI', 'å¼€æº']
    
    print("æ­£åœ¨è·å–æ–°é—»...")
    
    # è·å–Hacker News
    print("- è·å–Hacker Newsçƒ­é—¨æ•…äº‹...")
    hn_stories = aggregator.fetch_hackernews_top_stories(5)
    
    # è·å–GitHubäº‹ä»¶
    print("- è·å–GitHubå…¬å¼€äº‹ä»¶...")
    github_events = aggregator.fetch_github_events(5)
    
    # çˆ¬å–æ–°é—»ç½‘ç«™ï¼ˆä½¿ç”¨httpbinä½œä¸ºç¤ºä¾‹ï¼‰
    print("- çˆ¬å–æ–°é—»ç½‘ç«™...")
    crawled_news = aggregator.crawl_news_website("http://httpbin.org/html")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nç”ŸæˆæŠ¥å‘Š...\n")
    report = aggregator.format_report()
    
    if report:
        print(report)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = aggregator.generate_html_report()
        with open('news_report.html', 'w', encoding='utf-8') as f:
            f.write(html_report)
        print("\nHTMLæŠ¥å‘Šå·²ä¿å­˜åˆ° news_report.html")
    else:
        print("ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼")


# è¾…åŠ©å‡½æ•°
def format_number(num):
    """æ ¼å¼åŒ–å¤§æ•°å­—æ˜¾ç¤º"""
    if num >= 1000:
        return f"{num/1000:.1f}k"
    return str(num)


def clean_html(text):
    """æ¸…ç†HTMLæ ‡ç­¾"""
    return re.sub(r'<[^>]+>', '', text)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    main()
    
    # å•å…ƒæµ‹è¯•
    print("\n" + "="*50)
    print("å•å…ƒæµ‹è¯•ï¼š")
    
    aggregator = NewsAggregator()
    
    # æµ‹è¯•å…³é”®è¯æå–
    print("\næµ‹è¯•å…³é”®è¯æå–:")
    sample_texts = [
        "Python programming is awesome for data science",
        "Machine learning with Python and TensorFlow",
        "Python for web development using Django"
    ]
    keywords = aggregator.extract_keywords(sample_texts, 5)
    print(f"æå–çš„å…³é”®è¯: {keywords}")
    
    # æµ‹è¯•æƒ…æ„Ÿåˆ†æ
    print("\næµ‹è¯•æƒ…æ„Ÿåˆ†æ:")
    test_texts = [
        "This is a great product, I love it!",
        "Terrible experience, worst service ever",
        "It's okay, nothing special"
    ]
    for text in test_texts:
        sentiment = aggregator.analyze_sentiment(text)
        print(f"æ–‡æœ¬: \"{text}\"")
        print(f"æƒ…æ„Ÿ: {sentiment}")
    
    # æµ‹è¯•æ•°å­—æ ¼å¼åŒ–
    print("\næµ‹è¯•æ•°å­—æ ¼å¼åŒ–:")
    test_numbers = [123, 1234, 12345, 123456]
    for num in test_numbers:
        print(f"{num} -> {format_number(num)}") 