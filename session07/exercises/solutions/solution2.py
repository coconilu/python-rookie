#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Session07 ç»ƒä¹ 2è§£ç­”ï¼šCSVå’ŒJSONæ–‡ä»¶æ“ä½œ

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†ç»ƒä¹ 2çš„å®Œæ•´è§£ç­”ã€‚
å­¦ä¹ è€…å¯ä»¥å‚è€ƒè¿™äº›è§£ç­”æ¥ç†è§£æ­£ç¡®çš„å®ç°æ–¹æ³•ã€‚

ä½œè€…: Pythonæ•™ç¨‹å›¢é˜Ÿ
åˆ›å»ºæ—¥æœŸ: 2024-12-22
"""

import csv
import json
import os
import glob
import time
from datetime import datetime, timedelta
import random
from copy import deepcopy


def exercise1_create_employee_csv():
    """
    ç»ƒä¹ 1è§£ç­”ï¼šåˆ›å»ºå‘˜å·¥CSVæ–‡ä»¶
    """
    filename = 'employees.csv'
    
    # å‡†å¤‡å‘˜å·¥æ•°æ®
    employees = [
        {'å‘˜å·¥ID': 'E001', 'å§“å': 'å¼ ä¸‰', 'éƒ¨é—¨': 'æŠ€æœ¯éƒ¨', 'èŒä½': 'è½¯ä»¶å·¥ç¨‹å¸ˆ', 'è–ªèµ„': 12000, 'å…¥èŒæ—¥æœŸ': '2022-03-15'},
        {'å‘˜å·¥ID': 'E002', 'å§“å': 'æå››', 'éƒ¨é—¨': 'é”€å”®éƒ¨', 'èŒä½': 'é”€å”®ç»ç†', 'è–ªèµ„': 15000, 'å…¥èŒæ—¥æœŸ': '2021-08-20'},
        {'å‘˜å·¥ID': 'E003', 'å§“å': 'ç‹äº”', 'éƒ¨é—¨': 'äººäº‹éƒ¨', 'èŒä½': 'HRä¸“å‘˜', 'è–ªèµ„': 8000, 'å…¥èŒæ—¥æœŸ': '2023-01-10'},
        {'å‘˜å·¥ID': 'E004', 'å§“å': 'èµµå…­', 'éƒ¨é—¨': 'æŠ€æœ¯éƒ¨', 'èŒä½': 'å‰ç«¯å¼€å‘', 'è–ªèµ„': 10000, 'å…¥èŒæ—¥æœŸ': '2022-11-05'},
        {'å‘˜å·¥ID': 'E005', 'å§“å': 'é’±ä¸ƒ', 'éƒ¨é—¨': 'è´¢åŠ¡éƒ¨', 'èŒä½': 'ä¼šè®¡å¸ˆ', 'è–ªèµ„': 9000, 'å…¥èŒæ—¥æœŸ': '2021-12-01'},
        {'å‘˜å·¥ID': 'E006', 'å§“å': 'å­™å…«', 'éƒ¨é—¨': 'æŠ€æœ¯éƒ¨', 'èŒä½': 'æ¶æ„å¸ˆ', 'è–ªèµ„': 18000, 'å…¥èŒæ—¥æœŸ': '2020-06-15'},
        {'å‘˜å·¥ID': 'E007', 'å§“å': 'å‘¨ä¹', 'éƒ¨é—¨': 'å¸‚åœºéƒ¨', 'èŒä½': 'å¸‚åœºä¸“å‘˜', 'è–ªèµ„': 7500, 'å…¥èŒæ—¥æœŸ': '2023-04-20'},
        {'å‘˜å·¥ID': 'E008', 'å§“å': 'å´å', 'éƒ¨é—¨': 'é”€å”®éƒ¨', 'èŒä½': 'é”€å”®ä»£è¡¨', 'è–ªèµ„': 6500, 'å…¥èŒæ—¥æœŸ': '2023-02-28'},
        {'å‘˜å·¥ID': 'E009', 'å§“å': 'éƒ‘åä¸€', 'éƒ¨é—¨': 'æŠ€æœ¯éƒ¨', 'èŒä½': 'æµ‹è¯•å·¥ç¨‹å¸ˆ', 'è–ªèµ„': 9500, 'å…¥èŒæ—¥æœŸ': '2022-09-10'},
        {'å‘˜å·¥ID': 'E010', 'å§“å': 'ç‹åäºŒ', 'éƒ¨é—¨': 'è´¢åŠ¡éƒ¨', 'èŒä½': 'è´¢åŠ¡ç»ç†', 'è–ªèµ„': 14000, 'å…¥èŒæ—¥æœŸ': '2021-05-18'}
    ]
    
    # å†™å…¥CSVæ–‡ä»¶
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['å‘˜å·¥ID', 'å§“å', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å…¥èŒæ—¥æœŸ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # å†™å…¥æ ‡é¢˜è¡Œ
        writer.writeheader()
        
        # å†™å…¥æ•°æ®è¡Œ
        for employee in employees:
            writer.writerow(employee)
    
    print(f"âœ“ åˆ›å»ºå‘˜å·¥CSVæ–‡ä»¶ï¼š{filename}ï¼ŒåŒ…å« {len(employees)} ä¸ªå‘˜å·¥")
    return len(employees)


def exercise2_read_and_filter_csv():
    """
    ç»ƒä¹ 2è§£ç­”ï¼šè¯»å–å’Œç­›é€‰CSVæ•°æ®
    """
    input_filename = 'employees.csv'
    output_filename = 'high_salary_employees.csv'
    
    if not os.path.exists(input_filename):
        print(f"âŒ æ–‡ä»¶ {input_filename} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç»ƒä¹ 1")
        return None
    
    high_salary_employees = []
    
    # è¯»å–CSVæ–‡ä»¶
    with open(input_filename, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        
        for row in reader:
            salary = int(row['è–ªèµ„'])
            if salary >= 8000:  # ç­›é€‰é«˜è–ªå‘˜å·¥
                high_salary_employees.append(row)
    
    # æŒ‰è–ªèµ„ä»é«˜åˆ°ä½æ’åº
    high_salary_employees.sort(key=lambda x: int(x['è–ªèµ„']), reverse=True)
    
    # å†™å…¥æ–°çš„CSVæ–‡ä»¶
    if high_salary_employees:
        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['å‘˜å·¥ID', 'å§“å', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å…¥èŒæ—¥æœŸ']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for employee in high_salary_employees:
                writer.writerow(employee)
    
    # è®¡ç®—å¹³å‡è–ªèµ„
    if high_salary_employees:
        total_salary = sum(int(emp['è–ªèµ„']) for emp in high_salary_employees)
        avg_salary = total_salary / len(high_salary_employees)
    else:
        avg_salary = 0
    
    print(f"âœ“ ç­›é€‰å‡º {len(high_salary_employees)} ä¸ªé«˜è–ªå‘˜å·¥")
    print(f"âœ“ å¹³å‡è–ªèµ„ï¼š{avg_salary:.0f} å…ƒ")
    print(f"âœ“ ç»“æœä¿å­˜åˆ°ï¼š{output_filename}")
    
    return (len(high_salary_employees), avg_salary)


def exercise3_csv_data_analysis():
    """
    ç»ƒä¹ 3è§£ç­”ï¼šCSVæ•°æ®åˆ†æ
    """
    filename = 'employees.csv'
    output_filename = 'employee_analysis.json'
    
    if not os.path.exists(filename):
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç»ƒä¹ 1")
        return None
    
    employees = []
    
    # è¯»å–å‘˜å·¥æ•°æ®
    with open(filename, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            employees.append(row)
    
    # åˆ†ææ•°æ®
    analysis_result = {
        'analysis_date': datetime.now().isoformat(),
        'total_employees': len(employees),
        'department_analysis': {},
        'position_analysis': {},
        'hiring_year_analysis': {}
    }
    
    # æŒ‰éƒ¨é—¨åˆ†æ
    dept_stats = {}
    for emp in employees:
        dept = emp['éƒ¨é—¨']
        salary = int(emp['è–ªèµ„'])
        
        if dept not in dept_stats:
            dept_stats[dept] = {'count': 0, 'total_salary': 0, 'salaries': []}
        
        dept_stats[dept]['count'] += 1
        dept_stats[dept]['total_salary'] += salary
        dept_stats[dept]['salaries'].append(salary)
    
    # è®¡ç®—éƒ¨é—¨ç»Ÿè®¡ä¿¡æ¯
    for dept, stats in dept_stats.items():
        avg_salary = stats['total_salary'] / stats['count']
        analysis_result['department_analysis'][dept] = {
            'employee_count': stats['count'],
            'average_salary': round(avg_salary, 0),
            'total_salary': stats['total_salary']
        }
    
    # æŒ‰èŒä½åˆ†æè–ªèµ„èŒƒå›´
    position_stats = {}
    for emp in employees:
        position = emp['èŒä½']
        salary = int(emp['è–ªèµ„'])
        
        if position not in position_stats:
            position_stats[position] = []
        position_stats[position].append(salary)
    
    for position, salaries in position_stats.items():
        analysis_result['position_analysis'][position] = {
            'min_salary': min(salaries),
            'max_salary': max(salaries),
            'avg_salary': round(sum(salaries) / len(salaries), 0),
            'employee_count': len(salaries)
        }
    
    # æŒ‰å…¥èŒå¹´ä»½åˆ†æ
    year_stats = {}
    for emp in employees:
        hire_date = emp['å…¥èŒæ—¥æœŸ']
        year = hire_date.split('-')[0]
        year_stats[year] = year_stats.get(year, 0) + 1
    
    analysis_result['hiring_year_analysis'] = year_stats
    
    # ä¿å­˜åˆ†æç»“æœ
    with open(output_filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(analysis_result, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ æ•°æ®åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°ï¼š{output_filename}")
    print(f"âœ“ éƒ¨é—¨æ•°é‡ï¼š{len(analysis_result['department_analysis'])}")
    print(f"âœ“ èŒä½ç±»å‹ï¼š{len(analysis_result['position_analysis'])}")
    
    return analysis_result


def exercise4_create_product_json():
    """
    ç»ƒä¹ 4è§£ç­”ï¼šåˆ›å»ºäº§å“JSONæ–‡ä»¶
    """
    filename = 'products.json'
    
    # åˆ›å»ºäº§å“æ•°æ®ç»“æ„
    products_data = {
        'store_info': {
            'name': 'Pythonå­¦ä¹ å•†åº—',
            'created_at': datetime.now().isoformat(),
            'version': '1.0'
        },
        'categories': {
            'ç¼–ç¨‹ä¹¦ç±': {
                'id': 1,
                'description': 'å„ç§ç¼–ç¨‹è¯­è¨€å­¦ä¹ ä¹¦ç±',
                'products': [
                    {
                        'id': 'B001',
                        'name': 'Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ',
                        'price': 89.0,
                        'stock': 50,
                        'description': 'é€‚åˆåˆå­¦è€…çš„Pythonæ•™ç¨‹',
                        'tags': ['Python', 'å…¥é—¨', 'å®è·µ']
                    },
                    {
                        'id': 'B002',
                        'name': 'JavaScripté«˜çº§ç¨‹åºè®¾è®¡',
                        'price': 99.0,
                        'stock': 30,
                        'description': 'JavaScriptæƒå¨æŒ‡å—',
                        'tags': ['JavaScript', 'é«˜çº§', 'å‰ç«¯']
                    },
                    {
                        'id': 'B003',
                        'name': 'Javaæ ¸å¿ƒæŠ€æœ¯',
                        'price': 108.0,
                        'stock': 25,
                        'description': 'Javaå¼€å‘å¿…å¤‡å‚è€ƒä¹¦',
                        'tags': ['Java', 'æ ¸å¿ƒ', 'æŠ€æœ¯']
                    },
                    {
                        'id': 'B004',
                        'name': 'C++ç¨‹åºè®¾è®¡',
                        'price': 95.0,
                        'stock': 20,
                        'description': 'C++ç¼–ç¨‹ç»å…¸æ•™æ',
                        'tags': ['C++', 'ç¨‹åºè®¾è®¡', 'ç»å…¸']
                    },
                    {
                        'id': 'B005',
                        'name': 'Goè¯­è¨€å®æˆ˜',
                        'price': 85.0,
                        'stock': 35,
                        'description': 'Goè¯­è¨€å®é™…åº”ç”¨æŒ‡å—',
                        'tags': ['Go', 'å®æˆ˜', 'åº”ç”¨']
                    }
                ]
            },
            'åœ¨çº¿è¯¾ç¨‹': {
                'id': 2,
                'description': 'è§†é¢‘æ•™ç¨‹å’Œåœ¨çº¿åŸ¹è®­è¯¾ç¨‹',
                'products': [
                    {
                        'id': 'C001',
                        'name': 'Python Webå¼€å‘è¯¾ç¨‹',
                        'price': 299.0,
                        'stock': 100,
                        'description': 'ä»é›¶å¼€å§‹å­¦ä¹ Python Webå¼€å‘',
                        'tags': ['Python', 'Web', 'å¼€å‘', 'è§†é¢‘']
                    },
                    {
                        'id': 'C002',
                        'name': 'æ•°æ®åˆ†æå®æˆ˜è¯¾ç¨‹',
                        'price': 399.0,
                        'stock': 80,
                        'description': 'ä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æ',
                        'tags': ['æ•°æ®åˆ†æ', 'Python', 'å®æˆ˜']
                    },
                    {
                        'id': 'C003',
                        'name': 'æœºå™¨å­¦ä¹ å…¥é—¨',
                        'price': 499.0,
                        'stock': 60,
                        'description': 'æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºä¸å®è·µ',
                        'tags': ['æœºå™¨å­¦ä¹ ', 'AI', 'å…¥é—¨']
                    },
                    {
                        'id': 'C004',
                        'name': 'å‰ç«¯å¼€å‘å…¨æ ˆè¯¾ç¨‹',
                        'price': 599.0,
                        'stock': 40,
                        'description': 'HTML/CSS/JavaScriptå…¨æ ˆå¼€å‘',
                        'tags': ['å‰ç«¯', 'å…¨æ ˆ', 'HTML', 'CSS', 'JavaScript']
                    },
                    {
                        'id': 'C005',
                        'name': 'Dockerå®¹å™¨æŠ€æœ¯',
                        'price': 299.0,
                        'stock': 70,
                        'description': 'Dockerå®¹å™¨åŒ–éƒ¨ç½²å®æˆ˜',
                        'tags': ['Docker', 'å®¹å™¨', 'éƒ¨ç½²']
                    }
                ]
            },
            'å¼€å‘å·¥å…·': {
                'id': 3,
                'description': 'ç¼–ç¨‹å¼€å‘ç›¸å…³çš„è½¯ä»¶å’Œå·¥å…·',
                'products': [
                    {
                        'id': 'T001',
                        'name': 'PyCharmä¸“ä¸šç‰ˆ',
                        'price': 199.0,
                        'stock': 200,
                        'description': 'Pythoné›†æˆå¼€å‘ç¯å¢ƒ',
                        'tags': ['PyCharm', 'IDE', 'Python']
                    },
                    {
                        'id': 'T002',
                        'name': 'VS Codeæ’ä»¶åŒ…',
                        'price': 49.0,
                        'stock': 150,
                        'description': 'å®ç”¨çš„VS Codeæ‰©å±•æ’ä»¶é›†åˆ',
                        'tags': ['VS Code', 'æ’ä»¶', 'æ‰©å±•']
                    },
                    {
                        'id': 'T003',
                        'name': 'Gitç‰ˆæœ¬æ§åˆ¶å·¥å…·',
                        'price': 0.0,
                        'stock': 999,
                        'description': 'å…è´¹çš„ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ',
                        'tags': ['Git', 'ç‰ˆæœ¬æ§åˆ¶', 'å…è´¹']
                    },
                    {
                        'id': 'T004',
                        'name': 'Postman APIæµ‹è¯•å·¥å…·',
                        'price': 99.0,
                        'stock': 80,
                        'description': 'APIå¼€å‘å’Œæµ‹è¯•å·¥å…·',
                        'tags': ['Postman', 'API', 'æµ‹è¯•']
                    },
                    {
                        'id': 'T005',
                        'name': 'Sublime Textç¼–è¾‘å™¨',
                        'price': 80.0,
                        'stock': 120,
                        'description': 'è½»é‡çº§ä»£ç ç¼–è¾‘å™¨',
                        'tags': ['Sublime', 'ç¼–è¾‘å™¨', 'è½»é‡çº§']
                    }
                ]
            }
        }
    }
    
    # è®¡ç®—æ€»äº§å“æ•°
    total_products = 0
    for category in products_data['categories'].values():
        total_products += len(category['products'])
    
    products_data['statistics'] = {
        'total_products': total_products,
        'total_categories': len(products_data['categories']),
        'last_updated': datetime.now().isoformat()
    }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(products_data, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ åˆ›å»ºäº§å“JSONæ–‡ä»¶ï¼š{filename}")
    print(f"âœ“ æ€»äº§å“æ•°ï¼š{total_products}")
    print(f"âœ“ ç±»åˆ«æ•°ï¼š{len(products_data['categories'])}")
    
    return total_products


def exercise5_json_data_manipulation():
    """
    ç»ƒä¹ 5è§£ç­”ï¼šJSONæ•°æ®æ“ä½œ
    """
    filename = 'products.json'
    
    if not os.path.exists(filename):
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç»ƒä¹ 4")
        return None
    
    # è¯»å–JSONæ•°æ®
    with open(filename, 'r', encoding='utf-8') as jsonfile:
        products_data = json.load(jsonfile)
    
    # æ“ä½œç»Ÿè®¡
    stats = {'added': 0, 'updated': 0, 'deleted': 0}
    
    # 1. æ·»åŠ æ–°äº§å“
    new_products = [
        {
            'id': 'B006',
            'name': 'Rustç¼–ç¨‹è¯­è¨€',
            'price': 92.0,
            'stock': 15,
            'description': 'ç³»ç»Ÿç¼–ç¨‹è¯­è¨€Rustå…¥é—¨',
            'tags': ['Rust', 'ç³»ç»Ÿç¼–ç¨‹', 'å®‰å…¨']
        },
        {
            'id': 'C006',
            'name': 'åŒºå—é“¾å¼€å‘è¯¾ç¨‹',
            'price': 699.0,
            'stock': 30,
            'description': 'åŒºå—é“¾æŠ€æœ¯ä¸æ™ºèƒ½åˆçº¦å¼€å‘',
            'tags': ['åŒºå—é“¾', 'æ™ºèƒ½åˆçº¦', 'å¼€å‘']
        },
        {
            'id': 'T006',
            'name': 'IntelliJ IDEA',
            'price': 299.0,
            'stock': 90,
            'description': 'Javaé›†æˆå¼€å‘ç¯å¢ƒ',
            'tags': ['IntelliJ', 'IDE', 'Java']
        }
    ]
    
    # æ·»åŠ æ–°äº§å“åˆ°ç›¸åº”ç±»åˆ«
    products_data['categories']['ç¼–ç¨‹ä¹¦ç±']['products'].append(new_products[0])
    products_data['categories']['åœ¨çº¿è¯¾ç¨‹']['products'].append(new_products[1])
    products_data['categories']['å¼€å‘å·¥å…·']['products'].append(new_products[2])
    stats['added'] = len(new_products)
    
    # 2. æ›´æ–°äº§å“ä»·æ ¼ï¼ˆæ¶¨ä»·10%ï¼‰
    for category in products_data['categories'].values():
        for product in category['products']:
            if product['price'] > 0:  # è·³è¿‡å…è´¹äº§å“
                old_price = product['price']
                product['price'] = round(old_price * 1.1, 2)
                stats['updated'] += 1
    
    # 3. åˆ é™¤åº“å­˜ä¸º0çš„äº§å“
    for category in products_data['categories'].values():
        original_count = len(category['products'])
        category['products'] = [p for p in category['products'] if p['stock'] > 0]
        stats['deleted'] += original_count - len(category['products'])
    
    # 4. ä¸ºæ‰€æœ‰äº§å“æ·»åŠ last_updatedå­—æ®µ
    current_time = datetime.now().isoformat()
    for category in products_data['categories'].values():
        for product in category['products']:
            product['last_updated'] = current_time
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    total_products = sum(len(cat['products']) for cat in products_data['categories'].values())
    products_data['statistics']['total_products'] = total_products
    products_data['statistics']['last_updated'] = current_time
    
    # ä¿å­˜ä¿®æ”¹åçš„æ•°æ®
    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(products_data, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ JSONæ•°æ®æ“ä½œå®Œæˆ")
    print(f"âœ“ æ·»åŠ äº§å“ï¼š{stats['added']} ä¸ª")
    print(f"âœ“ æ›´æ–°äº§å“ï¼š{stats['updated']} ä¸ª")
    print(f"âœ“ åˆ é™¤äº§å“ï¼š{stats['deleted']} ä¸ª")
    
    return stats


def exercise6_csv_to_json_conversion():
    """
    ç»ƒä¹ 6è§£ç­”ï¼šCSVåˆ°JSONæ ¼å¼è½¬æ¢
    """
    input_filename = 'employees.csv'
    output_filename = 'employees_by_department.json'
    
    if not os.path.exists(input_filename):
        print(f"âŒ æ–‡ä»¶ {input_filename} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç»ƒä¹ 1")
        return None
    
    employees = []
    
    # è¯»å–CSVæ•°æ®
    with open(input_filename, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            employees.append(row)
    
    # æŒ‰éƒ¨é—¨åˆ†ç»„
    departments = {}
    for emp in employees:
        dept_name = emp['éƒ¨é—¨']
        if dept_name not in departments:
            departments[dept_name] = {
                'department_name': dept_name,
                'employees': [],
                'statistics': {
                    'employee_count': 0,
                    'total_salary': 0,
                    'average_salary': 0,
                    'min_salary': float('inf'),
                    'max_salary': 0
                }
            }
        
        # æ·»åŠ å‘˜å·¥ä¿¡æ¯
        employee_info = {
            'employee_id': emp['å‘˜å·¥ID'],
            'name': emp['å§“å'],
            'position': emp['èŒä½'],
            'salary': int(emp['è–ªèµ„']),
            'hire_date': emp['å…¥èŒæ—¥æœŸ']
        }
        
        departments[dept_name]['employees'].append(employee_info)
    
    # è®¡ç®—éƒ¨é—¨ç»Ÿè®¡ä¿¡æ¯
    for dept_name, dept_data in departments.items():
        employees_list = dept_data['employees']
        salaries = [emp['salary'] for emp in employees_list]
        
        dept_data['statistics']['employee_count'] = len(employees_list)
        dept_data['statistics']['total_salary'] = sum(salaries)
        dept_data['statistics']['average_salary'] = round(sum(salaries) / len(salaries), 0)
        dept_data['statistics']['min_salary'] = min(salaries)
        dept_data['statistics']['max_salary'] = max(salaries)
    
    # ç»„ç»‡æœ€ç»ˆæ•°æ®ç»“æ„
    result_data = {
        'conversion_info': {
            'source_file': input_filename,
            'conversion_date': datetime.now().isoformat(),
            'total_employees': len(employees),
            'total_departments': len(departments)
        },
        'departments': departments
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(output_filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(result_data, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ CSVè½¬JSONå®Œæˆï¼Œç»“æœä¿å­˜åˆ°ï¼š{output_filename}")
    print(f"âœ“ è½¬æ¢éƒ¨é—¨æ•°ï¼š{len(departments)}")
    print(f"âœ“ æ€»å‘˜å·¥æ•°ï¼š{len(employees)}")
    
    return len(departments)


def exercise7_data_validation():
    """
    ç»ƒä¹ 7è§£ç­”ï¼šæ•°æ®éªŒè¯å’Œæ¸…æ´—
    """
    dirty_filename = 'dirty_data.csv'
    clean_filename = 'clean_data.csv'
    report_filename = 'data_quality_report.json'
    
    # 1. åˆ›å»ºåŒ…å«é”™è¯¯æ•°æ®çš„CSVæ–‡ä»¶
    dirty_data = [
        ['å‘˜å·¥ID', 'å§“å', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å…¥èŒæ—¥æœŸ'],
        ['E001', 'å¼ ä¸‰', 'æŠ€æœ¯éƒ¨', 'è½¯ä»¶å·¥ç¨‹å¸ˆ', '12000', '2022-03-15'],
        ['E002', '', 'é”€å”®éƒ¨', 'é”€å”®ç»ç†', '15000', '2021-08-20'],  # ç¼ºå¤±å§“å
        ['E003', 'ç‹äº”', '', 'HRä¸“å‘˜', '8000', '2023-01-10'],  # ç¼ºå¤±éƒ¨é—¨
        ['E004', 'èµµå…­', 'æŠ€æœ¯éƒ¨', 'å‰ç«¯å¼€å‘', '-5000', '2022-11-05'],  # è´Ÿæ•°è–ªèµ„
        ['E005', 'é’±ä¸ƒ', 'è´¢åŠ¡éƒ¨', 'ä¼šè®¡å¸ˆ', '9000', '2021-13-01'],  # é”™è¯¯æ—¥æœŸ
        ['E006', 'å­™å…«', 'æŠ€æœ¯éƒ¨', 'æ¶æ„å¸ˆ', '18000', '2020-06-15'],
        ['E007', 'å‘¨ä¹', 'å¸‚åœºéƒ¨', 'å¸‚åœºä¸“å‘˜', 'abc', '2023-04-20'],  # éæ•°å­—è–ªèµ„
        ['E008', 'å´å', 'é”€å”®éƒ¨', 'é”€å”®ä»£è¡¨', '6500', ''],  # ç¼ºå¤±æ—¥æœŸ
        ['E009', 'éƒ‘åä¸€', 'æŠ€æœ¯éƒ¨', 'æµ‹è¯•å·¥ç¨‹å¸ˆ', '9500', '2022-09-10'],
        ['', 'ç‹åäºŒ', 'è´¢åŠ¡éƒ¨', 'è´¢åŠ¡ç»ç†', '14000', '2021-05-18'],  # ç¼ºå¤±ID
    ]
    
    # å†™å…¥è„æ•°æ®æ–‡ä»¶
    with open(dirty_filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(dirty_data)
    
    print(f"âœ“ åˆ›å»ºè„æ•°æ®æ–‡ä»¶ï¼š{dirty_filename}")
    
    # 2. è¯»å–å¹¶éªŒè¯æ•°æ®
    clean_records = []
    validation_errors = []
    
    with open(dirty_filename, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        
        for row_num, row in enumerate(reader, start=2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆè·³è¿‡æ ‡é¢˜ï¼‰
            errors = []
            
            # éªŒè¯å‘˜å·¥ID
            if not row['å‘˜å·¥ID'].strip():
                errors.append('å‘˜å·¥IDä¸ºç©º')
            
            # éªŒè¯å§“å
            if not row['å§“å'].strip():
                errors.append('å§“åä¸ºç©º')
                row['å§“å'] = f'å‘˜å·¥{row_num}'  # å¡«å……é»˜è®¤å€¼
            
            # éªŒè¯éƒ¨é—¨
            if not row['éƒ¨é—¨'].strip():
                errors.append('éƒ¨é—¨ä¸ºç©º')
                row['éƒ¨é—¨'] = 'æœªåˆ†é…éƒ¨é—¨'  # å¡«å……é»˜è®¤å€¼
            
            # éªŒè¯è–ªèµ„
            try:
                salary = int(row['è–ªèµ„'])
                if salary < 0:
                    errors.append('è–ªèµ„ä¸ºè´Ÿæ•°')
                    row['è–ªèµ„'] = '0'  # ä¿®æ­£ä¸º0
                elif salary > 100000:
                    errors.append('è–ªèµ„è¿‡é«˜ï¼Œå¯èƒ½æœ‰è¯¯')
            except ValueError:
                errors.append('è–ªèµ„æ ¼å¼é”™è¯¯')
                row['è–ªèµ„'] = '0'  # ä¿®æ­£ä¸º0
            
            # éªŒè¯æ—¥æœŸ
            if row['å…¥èŒæ—¥æœŸ'].strip():
                try:
                    datetime.strptime(row['å…¥èŒæ—¥æœŸ'], '%Y-%m-%d')
                except ValueError:
                    errors.append('æ—¥æœŸæ ¼å¼é”™è¯¯')
                    row['å…¥èŒæ—¥æœŸ'] = '2023-01-01'  # å¡«å……é»˜è®¤æ—¥æœŸ
            else:
                errors.append('å…¥èŒæ—¥æœŸä¸ºç©º')
                row['å…¥èŒæ—¥æœŸ'] = '2023-01-01'  # å¡«å……é»˜è®¤æ—¥æœŸ
            
            # è®°å½•é”™è¯¯
            if errors:
                validation_errors.append({
                    'row_number': row_num,
                    'employee_id': row['å‘˜å·¥ID'],
                    'errors': errors
                })
            
            # å¦‚æœå‘˜å·¥IDä¸ä¸ºç©ºï¼Œåˆ™ä¿ç•™è®°å½•ï¼ˆå³ä½¿æœ‰å…¶ä»–é”™è¯¯ä¹Ÿå°è¯•ä¿®å¤ï¼‰
            if row['å‘˜å·¥ID'].strip():
                clean_records.append(row)
    
    # 3. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    if clean_records:
        with open(clean_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['å‘˜å·¥ID', 'å§“å', 'éƒ¨é—¨', 'èŒä½', 'è–ªèµ„', 'å…¥èŒæ—¥æœŸ']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(clean_records)
    
    # 4. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    quality_report = {
        'validation_date': datetime.now().isoformat(),
        'source_file': dirty_filename,
        'clean_file': clean_filename,
        'summary': {
            'original_records': len(dirty_data) - 1,  # å‡å»æ ‡é¢˜è¡Œ
            'clean_records': len(clean_records),
            'removed_records': (len(dirty_data) - 1) - len(clean_records),
            'total_errors': len(validation_errors),
            'error_rate': round(len(validation_errors) / (len(dirty_data) - 1) * 100, 1)
        },
        'validation_errors': validation_errors,
        'data_quality_issues': {
            'missing_employee_id': sum(1 for e in validation_errors if any('å‘˜å·¥ID' in err for err in e['errors'])),
            'missing_name': sum(1 for e in validation_errors if any('å§“å' in err for err in e['errors'])),
            'missing_department': sum(1 for e in validation_errors if any('éƒ¨é—¨' in err for err in e['errors'])),
            'invalid_salary': sum(1 for e in validation_errors if any('è–ªèµ„' in err for err in e['errors'])),
            'invalid_date': sum(1 for e in validation_errors if any('æ—¥æœŸ' in err for err in e['errors']))
        }
    }
    
    # ä¿å­˜è´¨é‡æŠ¥å‘Š
    with open(report_filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(quality_report, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ æ•°æ®æ¸…æ´—å®Œæˆ")
    print(f"âœ“ åŸå§‹è®°å½•ï¼š{quality_report['summary']['original_records']} æ¡")
    print(f"âœ“ æ¸…æ´—åè®°å½•ï¼š{quality_report['summary']['clean_records']} æ¡")
    print(f"âœ“ é”™è¯¯ç‡ï¼š{quality_report['summary']['error_rate']}%")
    print(f"âœ“ è´¨é‡æŠ¥å‘Šä¿å­˜åˆ°ï¼š{report_filename}")
    
    return (quality_report['summary']['original_records'], quality_report['summary']['clean_records'])


def exercise8_batch_file_processing():
    """
    ç»ƒä¹ 8è§£ç­”ï¼šæ‰¹é‡æ–‡ä»¶å¤„ç†
    """
    # 1. åˆ›å»ºå¤šä¸ªæœˆä»½çš„é”€å”®æ•°æ®æ–‡ä»¶
    months = ['2024-01', '2024-02', '2024-03', '2024-04', '2024-05', '2024-06']
    products = ['ç¬”è®°æœ¬ç”µè„‘', 'é¼ æ ‡', 'é”®ç›˜', 'æ˜¾ç¤ºå™¨', 'è€³æœº', 'éŸ³å“']
    customers = ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«', 'å‘¨ä¹', 'å´å']
    
    created_files = []
    total_records = 0
    
    for month in months:
        filename = f'sales_{month}.csv'
        
        # ç”Ÿæˆè¯¥æœˆçš„é”€å”®æ•°æ®
        sales_data = []
        for _ in range(random.randint(50, 100)):  # æ¯æœˆ50-100æ¡è®°å½•
            record = {
                'è®¢å•ID': f'ORD{month.replace("-", "")}{random.randint(1000, 9999)}',
                'æ—¥æœŸ': f'{month}-{random.randint(1, 28):02d}',
                'å®¢æˆ·': random.choice(customers),
                'äº§å“': random.choice(products),
                'æ•°é‡': random.randint(1, 5),
                'å•ä»·': random.randint(100, 2000),
                'æ€»é‡‘é¢': 0  # å°†åœ¨ä¸‹é¢è®¡ç®—
            }
            record['æ€»é‡‘é¢'] = record['æ•°é‡'] * record['å•ä»·']
            sales_data.append(record)
        
        # å†™å…¥CSVæ–‡ä»¶
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['è®¢å•ID', 'æ—¥æœŸ', 'å®¢æˆ·', 'äº§å“', 'æ•°é‡', 'å•ä»·', 'æ€»é‡‘é¢']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(sales_data)
        
        created_files.append(filename)
        total_records += len(sales_data)
        print(f"âœ“ åˆ›å»ºé”€å”®æ•°æ®æ–‡ä»¶ï¼š{filename}ï¼ˆ{len(sales_data)}æ¡è®°å½•ï¼‰")
    
    # 2. æ‰¹é‡è¯»å–æ‰€æœ‰CSVæ–‡ä»¶
    all_sales_data = []
    
    for filename in glob.glob('sales_*.csv'):
        with open(filename, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # è½¬æ¢æ•°æ®ç±»å‹
                row['æ•°é‡'] = int(row['æ•°é‡'])
                row['å•ä»·'] = int(row['å•ä»·'])
                row['æ€»é‡‘é¢'] = int(row['æ€»é‡‘é¢'])
                all_sales_data.append(row)
    
    # 3. æ•°æ®åˆ†æ
    analysis_result = {
        'analysis_date': datetime.now().isoformat(),
        'data_period': f'{months[0]} åˆ° {months[-1]}',
        'total_records': len(all_sales_data),
        'total_revenue': sum(record['æ€»é‡‘é¢'] for record in all_sales_data),
        'monthly_trends': {},
        'top_products': {},
        'customer_analysis': {}
    }
    
    # æœˆåº¦è¶‹åŠ¿åˆ†æ
    monthly_sales = {}
    for record in all_sales_data:
        month = record['æ—¥æœŸ'][:7]  # æå–å¹´-æœˆ
        if month not in monthly_sales:
            monthly_sales[month] = {'revenue': 0, 'orders': 0}
        monthly_sales[month]['revenue'] += record['æ€»é‡‘é¢']
        monthly_sales[month]['orders'] += 1
    
    analysis_result['monthly_trends'] = monthly_sales
    
    # äº§å“é”€å”®åˆ†æ
    product_sales = {}
    for record in all_sales_data:
        product = record['äº§å“']
        if product not in product_sales:
            product_sales[product] = {'quantity': 0, 'revenue': 0, 'orders': 0}
        product_sales[product]['quantity'] += record['æ•°é‡']
        product_sales[product]['revenue'] += record['æ€»é‡‘é¢']
        product_sales[product]['orders'] += 1
    
    # æŒ‰é”€å”®é¢æ’åºè·å–çƒ­é”€äº§å“
    top_products = sorted(product_sales.items(), key=lambda x: x[1]['revenue'], reverse=True)
    analysis_result['top_products'] = dict(top_products)
    
    # å®¢æˆ·åˆ†æ
    customer_stats = {}
    for record in all_sales_data:
        customer = record['å®¢æˆ·']
        if customer not in customer_stats:
            customer_stats[customer] = {'total_spent': 0, 'order_count': 0, 'avg_order_value': 0}
        customer_stats[customer]['total_spent'] += record['æ€»é‡‘é¢']
        customer_stats[customer]['order_count'] += 1
    
    # è®¡ç®—å¹³å‡è®¢å•ä»·å€¼
    for customer, stats in customer_stats.items():
        stats['avg_order_value'] = round(stats['total_spent'] / stats['order_count'], 2)
    
    analysis_result['customer_analysis'] = customer_stats
    
    # 4. ä¿å­˜ç»¼åˆæŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰
    json_report_file = 'sales_comprehensive_report.json'
    with open(json_report_file, 'w', encoding='utf-8') as jsonfile:
        json.dump(analysis_result, jsonfile, ensure_ascii=False, indent=2)
    
    # 5. ä¿å­˜ç»¼åˆæŠ¥å‘Šï¼ˆCSVæ ¼å¼ï¼‰
    csv_report_file = 'sales_summary_report.csv'
    with open(csv_report_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # å†™å…¥æ€»ä½“ç»Ÿè®¡
        writer.writerow(['é”€å”®ç»¼åˆæŠ¥å‘Š'])
        writer.writerow(['åˆ†ææ—¥æœŸ', analysis_result['analysis_date']])
        writer.writerow(['æ•°æ®æœŸé—´', analysis_result['data_period']])
        writer.writerow(['æ€»è®°å½•æ•°', analysis_result['total_records']])
        writer.writerow(['æ€»é”€å”®é¢', analysis_result['total_revenue']])
        writer.writerow([])
        
        # å†™å…¥æœˆåº¦è¶‹åŠ¿
        writer.writerow(['æœˆåº¦é”€å”®è¶‹åŠ¿'])
        writer.writerow(['æœˆä»½', 'é”€å”®é¢', 'è®¢å•æ•°'])
        for month, data in monthly_sales.items():
            writer.writerow([month, data['revenue'], data['orders']])
        writer.writerow([])
        
        # å†™å…¥äº§å“é”€å”®æ’è¡Œ
        writer.writerow(['äº§å“é”€å”®æ’è¡Œ'])
        writer.writerow(['äº§å“', 'é”€å”®é¢', 'é”€å”®æ•°é‡', 'è®¢å•æ•°'])
        for product, data in top_products[:5]:  # å‰5å
            writer.writerow([product, data['revenue'], data['quantity'], data['orders']])
    
    print(f"\nâœ“ æ‰¹é‡æ–‡ä»¶å¤„ç†å®Œæˆ")
    print(f"âœ“ å¤„ç†æ–‡ä»¶æ•°ï¼š{len(created_files)}")
    print(f"âœ“ æ€»è®°å½•æ•°ï¼š{total_records}")
    print(f"âœ“ æ€»é”€å”®é¢ï¼š{analysis_result['total_revenue']:,} å…ƒ")
    print(f"âœ“ JSONæŠ¥å‘Šï¼š{json_report_file}")
    print(f"âœ“ CSVæŠ¥å‘Šï¼š{csv_report_file}")
    
    return (len(created_files), total_records)


def exercise9_json_schema_validation():
    """
    ç»ƒä¹ 9è§£ç­”ï¼šJSONæ•°æ®ç»“æ„éªŒè¯
    """
    # 1. å®šä¹‰äº§å“æ•°æ®çš„JSONæ¨¡å¼
    product_schema = {
        'required_fields': ['id', 'name', 'price', 'stock'],
        'optional_fields': ['description', 'tags', 'last_updated'],
        'field_types': {
            'id': str,
            'name': str,
            'price': (int, float),
            'stock': int,
            'description': str,
            'tags': list,
            'last_updated': str
        },
        'validation_rules': {
            'price': lambda x: x >= 0,
            'stock': lambda x: x >= 0,
            'name': lambda x: len(x.strip()) > 0
        }
    }
    
    def validate_product(product, schema):
        """éªŒè¯å•ä¸ªäº§å“æ•°æ®"""
        errors = []
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in schema['required_fields']:
            if field not in product:
                errors.append(f'ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}')
        
        # æ£€æŸ¥å­—æ®µç±»å‹
        for field, expected_type in schema['field_types'].items():
            if field in product:
                if not isinstance(product[field], expected_type):
                    errors.append(f'å­—æ®µ {field} ç±»å‹é”™è¯¯ï¼ŒæœŸæœ› {expected_type.__name__}ï¼Œå®é™… {type(product[field]).__name__}')
        
        # æ£€æŸ¥éªŒè¯è§„åˆ™
        for field, rule in schema['validation_rules'].items():
            if field in product:
                try:
                    if not rule(product[field]):
                        errors.append(f'å­—æ®µ {field} éªŒè¯å¤±è´¥')
                except Exception as e:
                    errors.append(f'å­—æ®µ {field} éªŒè¯æ—¶å‡ºé”™: {str(e)}')
        
        return errors
    
    # 2. åˆ›å»ºæµ‹è¯•æ•°æ®
    test_products = [
        # æ­£ç¡®çš„äº§å“æ•°æ®
        {
            'id': 'P001',
            'name': 'æµ‹è¯•äº§å“1',
            'price': 99.99,
            'stock': 10,
            'description': 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•äº§å“',
            'tags': ['æµ‹è¯•', 'äº§å“']
        },
        # ç¼ºå°‘å¿…éœ€å­—æ®µ
        {
            'name': 'æµ‹è¯•äº§å“2',
            'price': 199.99,
            'stock': 5
        },
        # ç±»å‹é”™è¯¯
        {
            'id': 'P003',
            'name': 'æµ‹è¯•äº§å“3',
            'price': 'å…è´¹',  # åº”è¯¥æ˜¯æ•°å­—
            'stock': 15
        },
        # éªŒè¯è§„åˆ™å¤±è´¥
        {
            'id': 'P004',
            'name': '',  # åç§°ä¸ºç©º
            'price': -50,  # è´Ÿä»·æ ¼
            'stock': -5  # è´Ÿåº“å­˜
        }
    ]
    
    # 3. éªŒè¯æµ‹è¯•æ•°æ®
    validation_results = []
    for i, product in enumerate(test_products):
        errors = validate_product(product, product_schema)
        validation_results.append({
            'product_index': i,
            'product_id': product.get('id', 'N/A'),
            'is_valid': len(errors) == 0,
            'errors': errors
        })
    
    # 4. éªŒè¯ç°æœ‰çš„products.jsonæ–‡ä»¶
    products_file = 'products.json'
    file_validation_result = None
    
    if os.path.exists(products_file):
        try:
            with open(products_file, 'r', encoding='utf-8') as jsonfile:
                products_data = json.load(jsonfile)
            
            file_errors = []
            valid_products = 0
            total_products = 0
            
            # éªŒè¯æ¯ä¸ªç±»åˆ«ä¸­çš„äº§å“
            for category_name, category_data in products_data.get('categories', {}).items():
                for product in category_data.get('products', []):
                    total_products += 1
                    errors = validate_product(product, product_schema)
                    if errors:
                        file_errors.append({
                            'category': category_name,
                            'product_id': product.get('id', 'N/A'),
                            'errors': errors
                        })
                    else:
                        valid_products += 1
            
            file_validation_result = {
                'file_name': products_file,
                'total_products': total_products,
                'valid_products': valid_products,
                'invalid_products': total_products - valid_products,
                'validation_rate': round(valid_products / total_products * 100, 1) if total_products > 0 else 0,
                'errors': file_errors
            }
            
        except Exception as e:
            file_validation_result = {
                'file_name': products_file,
                'error': f'è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'
            }
    
    # 5. ç”ŸæˆéªŒè¯æŠ¥å‘Š
    validation_report = {
        'validation_date': datetime.now().isoformat(),
        'schema_definition': product_schema,
        'test_data_validation': {
            'total_test_products': len(test_products),
            'valid_test_products': sum(1 for r in validation_results if r['is_valid']),
            'results': validation_results
        },
        'file_validation': file_validation_result
    }
    
    # ä¿å­˜éªŒè¯æŠ¥å‘Š
    report_file = 'json_validation_report.json'
    with open(report_file, 'w', encoding='utf-8') as jsonfile:
        json.dump(validation_report, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"âœ“ JSONæ¨¡å¼éªŒè¯å®Œæˆ")
    print(f"âœ“ æµ‹è¯•æ•°æ®éªŒè¯ï¼š{validation_report['test_data_validation']['valid_test_products']}/{validation_report['test_data_validation']['total_test_products']} é€šè¿‡")
    
    if file_validation_result and 'error' not in file_validation_result:
        print(f"âœ“ æ–‡ä»¶éªŒè¯ï¼š{file_validation_result['valid_products']}/{file_validation_result['total_products']} é€šè¿‡ï¼ˆ{file_validation_result['validation_rate']}%ï¼‰")
    
    print(f"âœ“ éªŒè¯æŠ¥å‘Šä¿å­˜åˆ°ï¼š{report_file}")
    
    return validation_report


def exercise10_performance_comparison():
    """
    ç»ƒä¹ 10è§£ç­”ï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    """
    # 1. ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
    test_data_size = 1000
    test_data = []
    
    for i in range(test_data_size):
        record = {
            'id': f'ID{i:04d}',
            'name': f'äº§å“{i}',
            'category': random.choice(['ç”µå­äº§å“', 'æœè£…', 'é£Ÿå“', 'å›¾ä¹¦', 'å®¶å…·']),
            'price': round(random.uniform(10, 1000), 2),
            'stock': random.randint(0, 100),
            'description': f'è¿™æ˜¯äº§å“{i}çš„è¯¦ç»†æè¿°ï¼ŒåŒ…å«ä¸€äº›éšæœºæ–‡æœ¬å†…å®¹ã€‚',
            'tags': [f'æ ‡ç­¾{j}' for j in range(random.randint(1, 5))],
            'created_at': (datetime.now() - timedelta(days=random.randint(0, 365))).isoformat()
        }
        test_data.append(record)
    
    print(f"âœ“ ç”Ÿæˆ {test_data_size} æ¡æµ‹è¯•æ•°æ®")
    
    # 2. æ€§èƒ½æµ‹è¯•å‡½æ•°
    def time_operation(operation_func, *args, **kwargs):
        """æµ‹é‡æ“ä½œæ‰§è¡Œæ—¶é—´"""
        start_time = time.time()
        result = operation_func(*args, **kwargs)
        end_time = time.time()
        return end_time - start_time, result
    
    # 3. CSVå†™å…¥æ€§èƒ½æµ‹è¯•
    def write_csv_data(data, filename):
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            if data:
                fieldnames = data[0].keys()
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)
    
    # 4. JSONå†™å…¥æ€§èƒ½æµ‹è¯•
    def write_json_data(data, filename):
        with open(filename, 'w', encoding='utf-8') as jsonfile:
            json.dump(data, jsonfile, ensure_ascii=False, indent=2)
    
    # 5. CSVè¯»å–æ€§èƒ½æµ‹è¯•
    def read_csv_data(filename):
        data = []
        with open(filename, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                data.append(row)
        return data
    
    # 6. JSONè¯»å–æ€§èƒ½æµ‹è¯•
    def read_json_data(filename):
        with open(filename, 'r', encoding='utf-8') as jsonfile:
            return json.load(jsonfile)
    
    # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    performance_results = {
        'test_date': datetime.now().isoformat(),
        'data_size': test_data_size,
        'results': {}
    }
    
    # CSVå†™å…¥æµ‹è¯•
    csv_filename = 'performance_test.csv'
    csv_write_time, _ = time_operation(write_csv_data, test_data, csv_filename)
    csv_file_size = os.path.getsize(csv_filename)
    
    # JSONå†™å…¥æµ‹è¯•
    json_filename = 'performance_test.json'
    json_write_time, _ = time_operation(write_json_data, test_data, json_filename)
    json_file_size = os.path.getsize(json_filename)
    
    # CSVè¯»å–æµ‹è¯•
    csv_read_time, csv_data = time_operation(read_csv_data, csv_filename)
    
    # JSONè¯»å–æµ‹è¯•
    json_read_time, json_data = time_operation(read_json_data, json_filename)
    
    # æ•´ç†æµ‹è¯•ç»“æœ
    performance_results['results'] = {
        'csv_performance': {
            'write_time': round(csv_write_time, 4),
            'read_time': round(csv_read_time, 4),
            'file_size': csv_file_size,
            'records_read': len(csv_data)
        },
        'json_performance': {
            'write_time': round(json_write_time, 4),
            'read_time': round(json_read_time, 4),
            'file_size': json_file_size,
            'records_read': len(json_data)
        },
        'comparison': {
            'write_speed_ratio': round(csv_write_time / json_write_time, 2),
            'read_speed_ratio': round(csv_read_time / json_read_time, 2),
            'size_ratio': round(csv_file_size / json_file_size, 2)
        }
    }
    
    # ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
    report_filename = 'performance_test_report.json'
    with open(report_filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(performance_results, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print(f"âœ“ CSVå†™å…¥æ—¶é—´ï¼š{csv_write_time:.4f}ç§’")
    print(f"âœ“ JSONå†™å…¥æ—¶é—´ï¼š{json_write_time:.4f}ç§’")
    print(f"âœ“ CSVè¯»å–æ—¶é—´ï¼š{csv_read_time:.4f}ç§’")
    print(f"âœ“ JSONè¯»å–æ—¶é—´ï¼š{json_read_time:.4f}ç§’")
    print(f"âœ“ CSVæ–‡ä»¶å¤§å°ï¼š{csv_file_size:,}å­—èŠ‚")
    print(f"âœ“ JSONæ–‡ä»¶å¤§å°ï¼š{json_file_size:,}å­—èŠ‚")
    print(f"âœ“ æ€§èƒ½æŠ¥å‘Šä¿å­˜åˆ°ï¼š{report_filename}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    os.remove(csv_filename)
    os.remove(json_filename)
    
    return performance_results


def cleanup_exercise_files():
    """
    æ¸…ç†ç»ƒä¹ ç”Ÿæˆçš„æ–‡ä»¶
    """
    files_to_clean = [
        'employees.csv',
        'high_salary_employees.csv',
        'employee_analysis.json',
        'products.json',
        'employees_by_department.json',
        'dirty_data.csv',
        'clean_data.csv',
        'data_quality_report.json',
        'sales_comprehensive_report.json',
        'sales_summary_report.csv',
        'json_validation_report.json',
        'performance_test_report.json'
    ]
    
    # æ¸…ç†æ‰¹é‡æ–‡ä»¶
    batch_files = glob.glob('sales_*.csv')
    files_to_clean.extend(batch_files)
    
    cleaned_count = 0
    for filename in files_to_clean:
        if os.path.exists(filename):
            os.remove(filename)
            cleaned_count += 1
            print(f"âœ“ åˆ é™¤æ–‡ä»¶: {filename}")
    
    print(f"\næ€»å…±æ¸…ç†äº† {cleaned_count} ä¸ªæ–‡ä»¶")


def run_all_exercises():
    """
    è¿è¡Œæ‰€æœ‰ç»ƒä¹ è§£ç­”
    """
    print("Session07 ç»ƒä¹ 2è§£ç­”ï¼šCSVå’ŒJSONæ–‡ä»¶æ“ä½œ")
    print("=" * 50)
    
    exercises = [
        ("ç»ƒä¹ 1ï¼šåˆ›å»ºå‘˜å·¥CSVæ–‡ä»¶", exercise1_create_employee_csv),
        ("ç»ƒä¹ 2ï¼šè¯»å–å’Œç­›é€‰CSVæ•°æ®", exercise2_read_and_filter_csv),
        ("ç»ƒä¹ 3ï¼šCSVæ•°æ®åˆ†æ", exercise3_csv_data_analysis),
        ("ç»ƒä¹ 4ï¼šåˆ›å»ºäº§å“JSONæ–‡ä»¶", exercise4_create_product_json),
        ("ç»ƒä¹ 5ï¼šJSONæ•°æ®æ“ä½œ", exercise5_json_data_manipulation),
        ("ç»ƒä¹ 6ï¼šCSVåˆ°JSONæ ¼å¼è½¬æ¢", exercise6_csv_to_json_conversion),
        ("ç»ƒä¹ 7ï¼šæ•°æ®éªŒè¯å’Œæ¸…æ´—", exercise7_data_validation),
        ("ç»ƒä¹ 8ï¼šæ‰¹é‡æ–‡ä»¶å¤„ç†", exercise8_batch_file_processing),
        ("ç»ƒä¹ 9ï¼šJSONæ•°æ®ç»“æ„éªŒè¯", exercise9_json_schema_validation),
        ("ç»ƒä¹ 10ï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•", exercise10_performance_comparison)
    ]
    
    for title, exercise_func in exercises:
        print(f"\n{title}")
        print("-" * 30)
        
        try:
            result = exercise_func()
            if result is not None:
                print(f"ç»“æœ: {result}")
            print("âœ… ç»ƒä¹ å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç»ƒä¹ å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 50)
    print("âœ… æ‰€æœ‰ç»ƒä¹ è§£ç­”è¿è¡Œå®Œæˆï¼")
    print("\nğŸ’¡ è§£ç­”è¦ç‚¹æ€»ç»“ï¼š")
    print("- CSVé€‚åˆè¡¨æ ¼æ•°æ®ï¼ŒJSONé€‚åˆç»“æ„åŒ–æ•°æ®")
    print("- ä½¿ç”¨csv.DictReader/DictWriterå¤„ç†CSVæ›´æ–¹ä¾¿")
    print("- JSONæ•°æ®æ“ä½œè¦æ³¨æ„æ•°æ®ç±»å‹å’Œç»“æ„")
    print("- æ•°æ®éªŒè¯å’Œæ¸…æ´—æ˜¯é‡è¦çš„æ•°æ®å¤„ç†æ­¥éª¤")
    print("- æ€§èƒ½æµ‹è¯•æœ‰åŠ©äºé€‰æ‹©åˆé€‚çš„æ•°æ®æ ¼å¼")
    print("- æ‰¹é‡å¤„ç†æ—¶è¦è€ƒè™‘å†…å­˜ä½¿ç”¨å’Œé”™è¯¯å¤„ç†")
    
    # è¯¢é—®æ˜¯å¦æ¸…ç†æ–‡ä»¶
    response = input("\næ˜¯å¦æ¸…ç†ç»ƒä¹ ç”Ÿæˆçš„æ–‡ä»¶ï¼Ÿ(y/n): ").lower().strip()
    if response == 'y':
        cleanup_exercise_files()
    else:
        print("ç»ƒä¹ æ–‡ä»¶å·²ä¿ç•™ï¼Œä½ å¯ä»¥ç»§ç»­åˆ†æå’Œå­¦ä¹ ã€‚")


if __name__ == "__main__":
    run_all_exercises()